{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "def train_ann(ann=None, dataloader=None, criterion=None, epochs=None):\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        i = 0\n",
    "        for data, target, index in dataloader:\n",
    "            #print(i)\n",
    "            #print(data)\n",
    "            # get the inputs; data is a list of [inputs, labels/target]\n",
    "            inputs = data\n",
    "            labels = target\n",
    "    \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # forward + backward + optimize\n",
    "            outputs = ann(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 20 == 19:    # print every 20 mini-batches\n",
    "                print('[epoch %d, pattern number %d] loss: %.3f' %\n",
    "                      (epoch + 1, index, running_loss / 20))\n",
    "                running_loss = 0.0\n",
    "            i += 1\n",
    "\n",
    "    print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        assert len(X) == len(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        data, target = self.X[i], self.y[i]\n",
    "        # to be able to get the pattern/example index later\n",
    "        index = i\n",
    "        return data,target,index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is one way to define a network\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer\n",
    "        self.predict = torch.nn.Linear(n_hidden, n_output)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x)) # activation function for hidden layer\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':    \n",
    "    torch.manual_seed(1)    # reproducible\n",
    "    \n",
    "    # x data (tensor), shape=(100, 1)\n",
    "    X = torch.unsqueeze(torch.linspace(-1, 1, 4000), dim=1)\n",
    "    # noisy y data (tensor), shape=(100, 1)\n",
    "    y = X.pow(2) + 0.2*torch.rand(X.size())\n",
    "    \n",
    "    dataset = MyDataset(X, y)\n",
    "    \n",
    "    # Set the k-fold\n",
    "    k = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Split the indices into k mutually exclusive subsets $\\mathcal{D}_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "    indices = range(len(dataset))\n",
    "    partitions = kf = KFold(n_splits=k, random_state=None, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error vector contains errors $e_i$ for every pattern $z^{(i)}$.\n",
    "The size of this vector in a sigle task scenario with continuos output\n",
    "(univariate regression) for a dataset with N pattern is (1 x N)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "    error_vector = np.arange(len(dataset))\n",
    "    predicted = np.arange(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>div.output_scroll { height: 44em; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in fold number: 1\n",
      "[epoch 1, pattern number 3393] loss: 0.137\n",
      "[epoch 1, pattern number 1138] loss: 0.130\n",
      "[epoch 1, pattern number 3594] loss: 0.072\n",
      "[epoch 1, pattern number 160] loss: 0.068\n",
      "[epoch 1, pattern number 3219] loss: 0.083\n",
      "[epoch 1, pattern number 1745] loss: 0.064\n",
      "[epoch 1, pattern number 1739] loss: 0.021\n",
      "[epoch 1, pattern number 1946] loss: 0.031\n",
      "[epoch 1, pattern number 3852] loss: 0.018\n",
      "[epoch 1, pattern number 2814] loss: 0.019\n",
      "[epoch 1, pattern number 1538] loss: 0.009\n",
      "[epoch 1, pattern number 892] loss: 0.029\n",
      "[epoch 1, pattern number 2758] loss: 0.014\n",
      "[epoch 1, pattern number 3084] loss: 0.007\n",
      "[epoch 1, pattern number 3550] loss: 0.004\n",
      "[epoch 1, pattern number 3670] loss: 0.015\n",
      "[epoch 1, pattern number 2251] loss: 0.005\n",
      "[epoch 1, pattern number 1352] loss: 0.005\n",
      "[epoch 1, pattern number 829] loss: 0.010\n",
      "[epoch 1, pattern number 519] loss: 0.007\n",
      "[epoch 1, pattern number 1669] loss: 0.014\n",
      "[epoch 1, pattern number 1785] loss: 0.005\n",
      "[epoch 1, pattern number 3981] loss: 0.007\n",
      "[epoch 1, pattern number 2925] loss: 0.009\n",
      "[epoch 1, pattern number 1822] loss: 0.005\n",
      "[epoch 1, pattern number 1069] loss: 0.009\n",
      "[epoch 1, pattern number 2525] loss: 0.004\n",
      "[epoch 1, pattern number 961] loss: 0.011\n",
      "[epoch 1, pattern number 2699] loss: 0.006\n",
      "[epoch 1, pattern number 2052] loss: 0.004\n",
      "[epoch 1, pattern number 2322] loss: 0.013\n",
      "[epoch 1, pattern number 229] loss: 0.010\n",
      "[epoch 1, pattern number 3685] loss: 0.012\n",
      "[epoch 1, pattern number 1788] loss: 0.007\n",
      "[epoch 1, pattern number 3479] loss: 0.011\n",
      "[epoch 1, pattern number 2174] loss: 0.011\n",
      "[epoch 1, pattern number 97] loss: 0.011\n",
      "[epoch 1, pattern number 477] loss: 0.009\n",
      "[epoch 1, pattern number 3620] loss: 0.011\n",
      "[epoch 1, pattern number 3087] loss: 0.010\n",
      "[epoch 1, pattern number 684] loss: 0.004\n",
      "[epoch 1, pattern number 1011] loss: 0.009\n",
      "[epoch 1, pattern number 1723] loss: 0.005\n",
      "[epoch 1, pattern number 3413] loss: 0.010\n",
      "[epoch 1, pattern number 1832] loss: 0.007\n",
      "[epoch 1, pattern number 703] loss: 0.016\n",
      "[epoch 1, pattern number 431] loss: 0.009\n",
      "[epoch 1, pattern number 2689] loss: 0.008\n",
      "[epoch 1, pattern number 3028] loss: 0.006\n",
      "[epoch 1, pattern number 3167] loss: 0.009\n",
      "[epoch 1, pattern number 784] loss: 0.010\n",
      "[epoch 1, pattern number 3248] loss: 0.006\n",
      "[epoch 1, pattern number 1430] loss: 0.011\n",
      "[epoch 1, pattern number 1915] loss: 0.008\n",
      "[epoch 1, pattern number 3488] loss: 0.009\n",
      "[epoch 1, pattern number 2581] loss: 0.006\n",
      "[epoch 1, pattern number 70] loss: 0.010\n",
      "[epoch 1, pattern number 164] loss: 0.009\n",
      "[epoch 1, pattern number 921] loss: 0.006\n",
      "[epoch 1, pattern number 1255] loss: 0.010\n",
      "[epoch 1, pattern number 1018] loss: 0.005\n",
      "[epoch 1, pattern number 3170] loss: 0.027\n",
      "[epoch 1, pattern number 3338] loss: 0.008\n",
      "[epoch 1, pattern number 2930] loss: 0.006\n",
      "[epoch 1, pattern number 3755] loss: 0.006\n",
      "[epoch 1, pattern number 2005] loss: 0.005\n",
      "[epoch 1, pattern number 593] loss: 0.005\n",
      "[epoch 1, pattern number 3427] loss: 0.006\n",
      "[epoch 1, pattern number 2718] loss: 0.008\n",
      "[epoch 1, pattern number 178] loss: 0.007\n",
      "[epoch 1, pattern number 697] loss: 0.007\n",
      "[epoch 1, pattern number 351] loss: 0.010\n",
      "[epoch 1, pattern number 2927] loss: 0.005\n",
      "[epoch 1, pattern number 2203] loss: 0.007\n",
      "[epoch 1, pattern number 3439] loss: 0.005\n",
      "[epoch 1, pattern number 3814] loss: 0.005\n",
      "[epoch 1, pattern number 759] loss: 0.006\n",
      "[epoch 1, pattern number 3754] loss: 0.006\n",
      "[epoch 1, pattern number 3626] loss: 0.007\n",
      "[epoch 1, pattern number 3407] loss: 0.011\n",
      "[epoch 1, pattern number 439] loss: 0.012\n",
      "[epoch 1, pattern number 1620] loss: 0.007\n",
      "[epoch 1, pattern number 3192] loss: 0.007\n",
      "[epoch 1, pattern number 719] loss: 0.011\n",
      "[epoch 1, pattern number 2332] loss: 0.007\n",
      "[epoch 1, pattern number 3412] loss: 0.011\n",
      "[epoch 1, pattern number 1425] loss: 0.006\n",
      "[epoch 1, pattern number 2011] loss: 0.008\n",
      "[epoch 1, pattern number 2441] loss: 0.008\n",
      "[epoch 1, pattern number 3012] loss: 0.007\n",
      "[epoch 1, pattern number 307] loss: 0.007\n",
      "[epoch 1, pattern number 1772] loss: 0.006\n",
      "[epoch 1, pattern number 3289] loss: 0.005\n",
      "[epoch 1, pattern number 594] loss: 0.006\n",
      "[epoch 1, pattern number 3221] loss: 0.007\n",
      "[epoch 1, pattern number 904] loss: 0.010\n",
      "[epoch 1, pattern number 2654] loss: 0.005\n",
      "[epoch 1, pattern number 471] loss: 0.009\n",
      "[epoch 1, pattern number 1205] loss: 0.007\n",
      "[epoch 1, pattern number 680] loss: 0.009\n",
      "[epoch 1, pattern number 2454] loss: 0.006\n",
      "[epoch 1, pattern number 3429] loss: 0.005\n",
      "[epoch 1, pattern number 3645] loss: 0.005\n",
      "[epoch 1, pattern number 1635] loss: 0.007\n",
      "[epoch 1, pattern number 2522] loss: 0.009\n",
      "[epoch 1, pattern number 1990] loss: 0.009\n",
      "[epoch 1, pattern number 2483] loss: 0.008\n",
      "[epoch 1, pattern number 2415] loss: 0.008\n",
      "[epoch 1, pattern number 1409] loss: 0.009\n",
      "[epoch 1, pattern number 2795] loss: 0.008\n",
      "[epoch 1, pattern number 736] loss: 0.007\n",
      "[epoch 1, pattern number 344] loss: 0.004\n",
      "[epoch 1, pattern number 107] loss: 0.007\n",
      "[epoch 1, pattern number 1093] loss: 0.010\n",
      "[epoch 1, pattern number 1584] loss: 0.007\n",
      "[epoch 1, pattern number 1871] loss: 0.004\n",
      "[epoch 1, pattern number 61] loss: 0.007\n",
      "[epoch 1, pattern number 708] loss: 0.010\n",
      "[epoch 1, pattern number 1861] loss: 0.005\n",
      "[epoch 1, pattern number 3284] loss: 0.007\n",
      "[epoch 1, pattern number 741] loss: 0.006\n",
      "[epoch 1, pattern number 1308] loss: 0.004\n",
      "[epoch 1, pattern number 3129] loss: 0.005\n",
      "[epoch 1, pattern number 831] loss: 0.007\n",
      "[epoch 1, pattern number 2169] loss: 0.006\n",
      "[epoch 1, pattern number 3229] loss: 0.006\n",
      "[epoch 1, pattern number 1560] loss: 0.007\n",
      "[epoch 1, pattern number 2246] loss: 0.006\n",
      "[epoch 1, pattern number 3851] loss: 0.009\n",
      "[epoch 1, pattern number 2779] loss: 0.006\n",
      "[epoch 1, pattern number 3554] loss: 0.006\n",
      "[epoch 1, pattern number 3651] loss: 0.006\n",
      "[epoch 1, pattern number 2837] loss: 0.007\n",
      "[epoch 2, pattern number 1205] loss: 0.004\n",
      "[epoch 2, pattern number 1533] loss: 0.004\n",
      "[epoch 2, pattern number 3002] loss: 0.013\n",
      "[epoch 2, pattern number 548] loss: 0.005\n",
      "[epoch 2, pattern number 2834] loss: 0.005\n",
      "[epoch 2, pattern number 3535] loss: 0.007\n",
      "[epoch 2, pattern number 1721] loss: 0.004\n",
      "[epoch 2, pattern number 2362] loss: 0.007\n",
      "[epoch 2, pattern number 3115] loss: 0.006\n",
      "[epoch 2, pattern number 2439] loss: 0.007\n",
      "[epoch 2, pattern number 127] loss: 0.008\n",
      "[epoch 2, pattern number 2227] loss: 0.006\n",
      "[epoch 2, pattern number 1280] loss: 0.013\n",
      "[epoch 2, pattern number 3916] loss: 0.007\n",
      "[epoch 2, pattern number 3082] loss: 0.007\n",
      "[epoch 2, pattern number 1750] loss: 0.004\n",
      "[epoch 2, pattern number 2474] loss: 0.012\n",
      "[epoch 2, pattern number 1118] loss: 0.009\n",
      "[epoch 2, pattern number 275] loss: 0.005\n",
      "[epoch 2, pattern number 299] loss: 0.005\n",
      "[epoch 2, pattern number 571] loss: 0.008\n",
      "[epoch 2, pattern number 1069] loss: 0.005\n",
      "[epoch 2, pattern number 1153] loss: 0.005\n",
      "[epoch 2, pattern number 2533] loss: 0.004\n",
      "[epoch 2, pattern number 321] loss: 0.004\n",
      "[epoch 2, pattern number 752] loss: 0.007\n",
      "[epoch 2, pattern number 2536] loss: 0.004\n",
      "[epoch 2, pattern number 3049] loss: 0.013\n",
      "[epoch 2, pattern number 83] loss: 0.008\n",
      "[epoch 2, pattern number 2630] loss: 0.011\n",
      "[epoch 2, pattern number 3318] loss: 0.005\n",
      "[epoch 2, pattern number 1946] loss: 0.009\n",
      "[epoch 2, pattern number 1134] loss: 0.008\n",
      "[epoch 2, pattern number 868] loss: 0.005\n",
      "[epoch 2, pattern number 1286] loss: 0.009\n",
      "[epoch 2, pattern number 3496] loss: 0.008\n",
      "[epoch 2, pattern number 1178] loss: 0.009\n",
      "[epoch 2, pattern number 3830] loss: 0.005\n",
      "[epoch 2, pattern number 3338] loss: 0.006\n",
      "[epoch 2, pattern number 3060] loss: 0.006\n",
      "[epoch 2, pattern number 3613] loss: 0.008\n",
      "[epoch 2, pattern number 271] loss: 0.005\n",
      "[epoch 2, pattern number 1858] loss: 0.005\n",
      "[epoch 2, pattern number 1483] loss: 0.008\n",
      "[epoch 2, pattern number 858] loss: 0.007\n",
      "[epoch 2, pattern number 3113] loss: 0.006\n",
      "[epoch 2, pattern number 1973] loss: 0.007\n",
      "[epoch 2, pattern number 2231] loss: 0.006\n",
      "[epoch 2, pattern number 3540] loss: 0.009\n",
      "[epoch 2, pattern number 1744] loss: 0.007\n",
      "[epoch 2, pattern number 1377] loss: 0.008\n",
      "[epoch 2, pattern number 214] loss: 0.005\n",
      "[epoch 2, pattern number 3619] loss: 0.007\n",
      "[epoch 2, pattern number 2392] loss: 0.006\n",
      "[epoch 2, pattern number 892] loss: 0.007\n",
      "[epoch 2, pattern number 1968] loss: 0.007\n",
      "[epoch 2, pattern number 2649] loss: 0.009\n",
      "[epoch 2, pattern number 1836] loss: 0.005\n",
      "[epoch 2, pattern number 1514] loss: 0.005\n",
      "[epoch 2, pattern number 1869] loss: 0.004\n",
      "[epoch 2, pattern number 3390] loss: 0.007\n",
      "[epoch 2, pattern number 3159] loss: 0.011\n",
      "[epoch 2, pattern number 1043] loss: 0.008\n",
      "[epoch 2, pattern number 1478] loss: 0.008\n",
      "[epoch 2, pattern number 1277] loss: 0.007\n",
      "[epoch 2, pattern number 3670] loss: 0.006\n",
      "[epoch 2, pattern number 1874] loss: 0.007\n",
      "[epoch 2, pattern number 606] loss: 0.004\n",
      "[epoch 2, pattern number 1511] loss: 0.006\n",
      "[epoch 2, pattern number 3069] loss: 0.007\n",
      "[epoch 2, pattern number 2594] loss: 0.004\n",
      "[epoch 2, pattern number 1337] loss: 0.006\n",
      "[epoch 2, pattern number 3631] loss: 0.006\n",
      "[epoch 2, pattern number 339] loss: 0.006\n",
      "[epoch 2, pattern number 2004] loss: 0.004\n",
      "[epoch 2, pattern number 393] loss: 0.004\n",
      "[epoch 2, pattern number 3173] loss: 0.011\n",
      "[epoch 2, pattern number 710] loss: 0.007\n",
      "[epoch 2, pattern number 1884] loss: 0.009\n",
      "[epoch 2, pattern number 3985] loss: 0.012\n",
      "[epoch 2, pattern number 2525] loss: 0.004\n",
      "[epoch 2, pattern number 997] loss: 0.008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 2, pattern number 2022] loss: 0.007\n",
      "[epoch 2, pattern number 903] loss: 0.006\n",
      "[epoch 2, pattern number 3575] loss: 0.005\n",
      "[epoch 2, pattern number 3708] loss: 0.006\n",
      "[epoch 2, pattern number 203] loss: 0.009\n",
      "[epoch 2, pattern number 2433] loss: 0.007\n",
      "[epoch 2, pattern number 914] loss: 0.006\n",
      "[epoch 2, pattern number 3657] loss: 0.005\n",
      "[epoch 2, pattern number 1522] loss: 0.007\n",
      "[epoch 2, pattern number 2186] loss: 0.007\n",
      "[epoch 2, pattern number 3664] loss: 0.003\n",
      "[epoch 2, pattern number 2199] loss: 0.006\n",
      "[epoch 2, pattern number 3143] loss: 0.007\n",
      "[epoch 2, pattern number 1437] loss: 0.005\n",
      "[epoch 2, pattern number 2974] loss: 0.006\n",
      "[epoch 2, pattern number 1028] loss: 0.007\n",
      "[epoch 2, pattern number 694] loss: 0.005\n",
      "[epoch 2, pattern number 2140] loss: 0.006\n",
      "[epoch 2, pattern number 1621] loss: 0.007\n",
      "[epoch 2, pattern number 1857] loss: 0.010\n",
      "[epoch 2, pattern number 811] loss: 0.006\n",
      "[epoch 2, pattern number 1471] loss: 0.004\n",
      "[epoch 2, pattern number 3219] loss: 0.006\n",
      "[epoch 2, pattern number 2158] loss: 0.006\n",
      "[epoch 2, pattern number 3185] loss: 0.006\n",
      "[epoch 2, pattern number 3719] loss: 0.005\n",
      "[epoch 2, pattern number 1673] loss: 0.009\n",
      "[epoch 2, pattern number 311] loss: 0.010\n",
      "[epoch 2, pattern number 2470] loss: 0.009\n",
      "[epoch 2, pattern number 701] loss: 0.008\n",
      "[epoch 2, pattern number 580] loss: 0.010\n",
      "[epoch 2, pattern number 3880] loss: 0.010\n",
      "[epoch 2, pattern number 2771] loss: 0.008\n",
      "[epoch 2, pattern number 91] loss: 0.005\n",
      "[epoch 2, pattern number 3605] loss: 0.005\n",
      "[epoch 2, pattern number 1423] loss: 0.008\n",
      "[epoch 2, pattern number 3136] loss: 0.006\n",
      "[epoch 2, pattern number 642] loss: 0.008\n",
      "[epoch 2, pattern number 3848] loss: 0.006\n",
      "[epoch 2, pattern number 281] loss: 0.006\n",
      "[epoch 2, pattern number 3747] loss: 0.009\n",
      "[epoch 2, pattern number 130] loss: 0.010\n",
      "[epoch 2, pattern number 2337] loss: 0.006\n",
      "[epoch 2, pattern number 34] loss: 0.010\n",
      "[epoch 2, pattern number 3866] loss: 0.006\n",
      "[epoch 2, pattern number 3508] loss: 0.004\n",
      "[epoch 2, pattern number 2912] loss: 0.011\n",
      "[epoch 2, pattern number 2535] loss: 0.012\n",
      "[epoch 2, pattern number 2483] loss: 0.008\n",
      "[epoch 2, pattern number 2669] loss: 0.005\n",
      "[epoch 2, pattern number 383] loss: 0.004\n",
      "Finished Training\n",
      "Validating in fold number: 1\n",
      "[fold number 1, pattern number 1419] current (single pattern) loss: 0.009\n",
      "[fold number 1, pattern number 2599] current (single pattern) loss: 0.000\n",
      "[fold number 1, pattern number 2979] current (single pattern) loss: 0.002\n",
      "[fold number 1, pattern number 3279] current (single pattern) loss: 0.000\n",
      "[fold number 1, pattern number 1999] current (single pattern) loss: 0.012\n",
      "[fold number 1, pattern number 3139] current (single pattern) loss: 0.004\n",
      "[fold number 1, pattern number 3659] current (single pattern) loss: 0.000\n",
      "[fold number 1, pattern number 1459] current (single pattern) loss: 0.010\n",
      "[fold number 1, pattern number 2119] current (single pattern) loss: 0.017\n",
      "[fold number 1, pattern number 1659] current (single pattern) loss: 0.001\n",
      "[fold number 1, pattern number 2479] current (single pattern) loss: 0.008\n",
      "[fold number 1, pattern number 3979] current (single pattern) loss: 0.007\n",
      "[fold number 1, pattern number 899] current (single pattern) loss: 0.000\n",
      "[fold number 1, pattern number 1679] current (single pattern) loss: 0.002\n",
      "[fold number 1, pattern number 3199] current (single pattern) loss: 0.000\n",
      "[fold number 1, pattern number 3059] current (single pattern) loss: 0.013\n",
      "[fold number 1, pattern number 2799] current (single pattern) loss: 0.002\n",
      "[fold number 1, pattern number 3379] current (single pattern) loss: 0.001\n",
      "[fold number 1, pattern number 139] current (single pattern) loss: 0.030\n",
      "[fold number 1, pattern number 619] current (single pattern) loss: 0.001\n",
      "[fold number 1, pattern number 599] current (single pattern) loss: 0.001\n",
      "[fold number 1, pattern number 1339] current (single pattern) loss: 0.000\n",
      "[fold number 1, pattern number 3799] current (single pattern) loss: 0.003\n",
      "[fold number 1, pattern number 1879] current (single pattern) loss: 0.015\n",
      "[fold number 1, pattern number 2919] current (single pattern) loss: 0.002\n",
      "[fold number 1, pattern number 1379] current (single pattern) loss: 0.003\n",
      "[fold number 1, pattern number 419] current (single pattern) loss: 0.016\n",
      "[fold number 1, pattern number 2819] current (single pattern) loss: 0.010\n",
      "[fold number 1, pattern number 1799] current (single pattern) loss: 0.003\n",
      "[fold number 1, pattern number 579] current (single pattern) loss: 0.010\n",
      "[fold number 1, pattern number 3119] current (single pattern) loss: 0.001\n",
      "[fold number 1, pattern number 2319] current (single pattern) loss: 0.004\n",
      "[fold number 1, pattern number 1819] current (single pattern) loss: 0.000\n",
      "[fold number 1, pattern number 2859] current (single pattern) loss: 0.015\n",
      "[fold number 1, pattern number 919] current (single pattern) loss: 0.002\n",
      "[fold number 1, pattern number 79] current (single pattern) loss: 0.001\n",
      "[fold number 1, pattern number 3519] current (single pattern) loss: 0.000\n",
      "[fold number 1, pattern number 119] current (single pattern) loss: 0.026\n",
      "[fold number 1, pattern number 2399] current (single pattern) loss: 0.000\n",
      "[fold number 1, pattern number 879] current (single pattern) loss: 0.010\n",
      "[fold number 1, pattern number 3679] current (single pattern) loss: 0.002\n",
      "[fold number 1, pattern number 2279] current (single pattern) loss: 0.016\n",
      "[fold number 1, pattern number 1019] current (single pattern) loss: 0.010\n",
      "[fold number 1, pattern number 1939] current (single pattern) loss: 0.019\n",
      "[fold number 1, pattern number 1899] current (single pattern) loss: 0.002\n",
      "[fold number 1, pattern number 659] current (single pattern) loss: 0.001\n",
      "[fold number 1, pattern number 3339] current (single pattern) loss: 0.004\n",
      "[fold number 1, pattern number 539] current (single pattern) loss: 0.000\n",
      "[fold number 1, pattern number 2059] current (single pattern) loss: 0.001\n",
      "[fold number 1, pattern number 3859] current (single pattern) loss: 0.001\n",
      "[fold number 1, pattern number 499] current (single pattern) loss: 0.002\n",
      "[fold number 1, pattern number 1839] current (single pattern) loss: 0.011\n",
      "[fold number 1, pattern number 639] current (single pattern) loss: 0.014\n",
      "[fold number 1, pattern number 1639] current (single pattern) loss: 0.005\n",
      "[fold number 1, pattern number 1199] current (single pattern) loss: 0.000\n",
      "[fold number 1, pattern number 1219] current (single pattern) loss: 0.011\n",
      "[fold number 1, pattern number 1719] current (single pattern) loss: 0.004\n",
      "[fold number 1, pattern number 2519] current (single pattern) loss: 0.003\n",
      "[fold number 1, pattern number 2019] current (single pattern) loss: 0.023\n",
      "[fold number 1, pattern number 3959] current (single pattern) loss: 0.002\n",
      "Training in fold number: 2\n",
      "[epoch 1, pattern number 2276] loss: 0.155\n",
      "[epoch 1, pattern number 3462] loss: 0.129\n",
      "[epoch 1, pattern number 3010] loss: 0.046\n",
      "[epoch 1, pattern number 1741] loss: 0.019\n",
      "[epoch 1, pattern number 2212] loss: 0.026\n",
      "[epoch 1, pattern number 3439] loss: 0.021\n",
      "[epoch 1, pattern number 1793] loss: 0.015\n",
      "[epoch 1, pattern number 2209] loss: 0.014\n",
      "[epoch 1, pattern number 322] loss: 0.008\n",
      "[epoch 1, pattern number 2752] loss: 0.009\n",
      "[epoch 1, pattern number 3694] loss: 0.004\n",
      "[epoch 1, pattern number 1587] loss: 0.010\n",
      "[epoch 1, pattern number 3799] loss: 0.014\n",
      "[epoch 1, pattern number 2555] loss: 0.016\n",
      "[epoch 1, pattern number 3746] loss: 0.010\n",
      "[epoch 1, pattern number 1069] loss: 0.008\n",
      "[epoch 1, pattern number 1087] loss: 0.011\n",
      "[epoch 1, pattern number 2188] loss: 0.007\n",
      "[epoch 1, pattern number 2831] loss: 0.007\n",
      "[epoch 1, pattern number 1868] loss: 0.005\n",
      "[epoch 1, pattern number 811] loss: 0.008\n",
      "[epoch 1, pattern number 3385] loss: 0.012\n",
      "[epoch 1, pattern number 506] loss: 0.007\n",
      "[epoch 1, pattern number 3647] loss: 0.005\n",
      "[epoch 1, pattern number 3420] loss: 0.012\n",
      "[epoch 1, pattern number 2004] loss: 0.005\n",
      "[epoch 1, pattern number 70] loss: 0.011\n",
      "[epoch 1, pattern number 480] loss: 0.013\n",
      "[epoch 1, pattern number 2077] loss: 0.005\n",
      "[epoch 1, pattern number 1624] loss: 0.003\n",
      "[epoch 1, pattern number 455] loss: 0.006\n",
      "[epoch 1, pattern number 2719] loss: 0.007\n",
      "[epoch 1, pattern number 1149] loss: 0.010\n",
      "[epoch 1, pattern number 1535] loss: 0.008\n",
      "[epoch 1, pattern number 3466] loss: 0.008\n",
      "[epoch 1, pattern number 2113] loss: 0.011\n",
      "[epoch 1, pattern number 2410] loss: 0.008\n",
      "[epoch 1, pattern number 3228] loss: 0.005\n",
      "[epoch 1, pattern number 1583] loss: 0.005\n",
      "[epoch 1, pattern number 3375] loss: 0.007\n",
      "[epoch 1, pattern number 3008] loss: 0.008\n",
      "[epoch 1, pattern number 2644] loss: 0.004\n",
      "[epoch 1, pattern number 2713] loss: 0.006\n",
      "[epoch 1, pattern number 895] loss: 0.006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1, pattern number 2776] loss: 0.005\n",
      "[epoch 1, pattern number 2705] loss: 0.009\n",
      "[epoch 1, pattern number 568] loss: 0.008\n",
      "[epoch 1, pattern number 3856] loss: 0.005\n",
      "[epoch 1, pattern number 3139] loss: 0.004\n",
      "[epoch 1, pattern number 546] loss: 0.012\n",
      "[epoch 1, pattern number 3204] loss: 0.008\n",
      "[epoch 1, pattern number 1294] loss: 0.007\n",
      "[epoch 1, pattern number 227] loss: 0.006\n",
      "[epoch 1, pattern number 3763] loss: 0.007\n",
      "[epoch 1, pattern number 2847] loss: 0.007\n",
      "[epoch 1, pattern number 1206] loss: 0.010\n",
      "[epoch 1, pattern number 926] loss: 0.006\n",
      "[epoch 1, pattern number 954] loss: 0.009\n",
      "[epoch 1, pattern number 1821] loss: 0.004\n",
      "[epoch 1, pattern number 3417] loss: 0.005\n",
      "[epoch 1, pattern number 3784] loss: 0.006\n",
      "[epoch 1, pattern number 1691] loss: 0.008\n",
      "[epoch 1, pattern number 3393] loss: 0.008\n",
      "[epoch 1, pattern number 801] loss: 0.007\n",
      "[epoch 1, pattern number 2730] loss: 0.006\n",
      "[epoch 1, pattern number 3540] loss: 0.004\n",
      "[epoch 1, pattern number 1447] loss: 0.005\n",
      "[epoch 1, pattern number 2550] loss: 0.008\n",
      "[epoch 1, pattern number 1311] loss: 0.005\n",
      "[epoch 1, pattern number 2452] loss: 0.006\n",
      "[epoch 1, pattern number 240] loss: 0.007\n",
      "[epoch 1, pattern number 462] loss: 0.008\n",
      "[epoch 1, pattern number 1204] loss: 0.007\n",
      "[epoch 1, pattern number 2940] loss: 0.003\n",
      "[epoch 1, pattern number 3918] loss: 0.009\n",
      "[epoch 1, pattern number 3133] loss: 0.008\n",
      "[epoch 1, pattern number 764] loss: 0.005\n",
      "[epoch 1, pattern number 2992] loss: 0.005\n",
      "[epoch 1, pattern number 2351] loss: 0.008\n",
      "[epoch 1, pattern number 515] loss: 0.006\n",
      "[epoch 1, pattern number 3219] loss: 0.010\n",
      "[epoch 1, pattern number 993] loss: 0.004\n",
      "[epoch 1, pattern number 553] loss: 0.006\n",
      "[epoch 1, pattern number 1514] loss: 0.007\n",
      "[epoch 1, pattern number 3456] loss: 0.010\n",
      "[epoch 1, pattern number 927] loss: 0.007\n",
      "[epoch 1, pattern number 1979] loss: 0.009\n",
      "[epoch 1, pattern number 114] loss: 0.003\n",
      "[epoch 1, pattern number 3264] loss: 0.005\n",
      "[epoch 1, pattern number 1223] loss: 0.006\n",
      "[epoch 1, pattern number 2363] loss: 0.008\n",
      "[epoch 1, pattern number 3244] loss: 0.006\n",
      "[epoch 1, pattern number 39] loss: 0.005\n",
      "[epoch 1, pattern number 2643] loss: 0.007\n",
      "[epoch 1, pattern number 275] loss: 0.009\n",
      "[epoch 1, pattern number 1341] loss: 0.007\n",
      "[epoch 1, pattern number 3503] loss: 0.006\n",
      "[epoch 1, pattern number 3245] loss: 0.016\n",
      "[epoch 1, pattern number 3089] loss: 0.004\n",
      "[epoch 1, pattern number 1879] loss: 0.006\n",
      "[epoch 1, pattern number 1404] loss: 0.004\n",
      "[epoch 1, pattern number 500] loss: 0.005\n",
      "[epoch 1, pattern number 2658] loss: 0.012\n",
      "[epoch 1, pattern number 2970] loss: 0.008\n",
      "[epoch 1, pattern number 3645] loss: 0.006\n",
      "[epoch 1, pattern number 1176] loss: 0.007\n",
      "[epoch 1, pattern number 132] loss: 0.006\n",
      "[epoch 1, pattern number 1991] loss: 0.006\n",
      "[epoch 1, pattern number 1413] loss: 0.004\n",
      "[epoch 1, pattern number 2274] loss: 0.006\n",
      "[epoch 1, pattern number 798] loss: 0.005\n",
      "[epoch 1, pattern number 3649] loss: 0.008\n",
      "[epoch 1, pattern number 488] loss: 0.011\n",
      "[epoch 1, pattern number 1885] loss: 0.008\n",
      "[epoch 1, pattern number 1004] loss: 0.005\n",
      "[epoch 1, pattern number 3200] loss: 0.007\n",
      "[epoch 1, pattern number 3578] loss: 0.011\n",
      "[epoch 1, pattern number 681] loss: 0.010\n",
      "[epoch 1, pattern number 3910] loss: 0.004\n",
      "[epoch 1, pattern number 1122] loss: 0.006\n",
      "[epoch 1, pattern number 353] loss: 0.004\n",
      "[epoch 1, pattern number 637] loss: 0.007\n",
      "[epoch 1, pattern number 1301] loss: 0.007\n",
      "[epoch 1, pattern number 481] loss: 0.004\n",
      "[epoch 1, pattern number 377] loss: 0.005\n",
      "[epoch 1, pattern number 516] loss: 0.009\n",
      "[epoch 1, pattern number 2665] loss: 0.006\n",
      "[epoch 1, pattern number 2486] loss: 0.009\n",
      "[epoch 1, pattern number 2261] loss: 0.009\n",
      "[epoch 1, pattern number 228] loss: 0.004\n",
      "[epoch 1, pattern number 475] loss: 0.008\n",
      "[epoch 1, pattern number 40] loss: 0.008\n",
      "[epoch 1, pattern number 1500] loss: 0.006\n",
      "[epoch 2, pattern number 210] loss: 0.005\n",
      "[epoch 2, pattern number 1914] loss: 0.009\n",
      "[epoch 2, pattern number 712] loss: 0.007\n",
      "[epoch 2, pattern number 1545] loss: 0.006\n",
      "[epoch 2, pattern number 3468] loss: 0.006\n",
      "[epoch 2, pattern number 226] loss: 0.004\n",
      "[epoch 2, pattern number 2281] loss: 0.010\n",
      "[epoch 2, pattern number 10] loss: 0.009\n",
      "[epoch 2, pattern number 641] loss: 0.007\n",
      "[epoch 2, pattern number 60] loss: 0.008\n",
      "[epoch 2, pattern number 2176] loss: 0.010\n",
      "[epoch 2, pattern number 1789] loss: 0.007\n",
      "[epoch 2, pattern number 2838] loss: 0.007\n",
      "[epoch 2, pattern number 756] loss: 0.007\n",
      "[epoch 2, pattern number 2432] loss: 0.006\n",
      "[epoch 2, pattern number 3092] loss: 0.006\n",
      "[epoch 2, pattern number 330] loss: 0.002\n",
      "[epoch 2, pattern number 453] loss: 0.003\n",
      "[epoch 2, pattern number 1607] loss: 0.005\n",
      "[epoch 2, pattern number 3482] loss: 0.008\n",
      "[epoch 2, pattern number 3511] loss: 0.007\n",
      "[epoch 2, pattern number 2275] loss: 0.005\n",
      "[epoch 2, pattern number 2775] loss: 0.004\n",
      "[epoch 2, pattern number 801] loss: 0.005\n",
      "[epoch 2, pattern number 1737] loss: 0.009\n",
      "[epoch 2, pattern number 3660] loss: 0.010\n",
      "[epoch 2, pattern number 1365] loss: 0.005\n",
      "[epoch 2, pattern number 3818] loss: 0.004\n",
      "[epoch 2, pattern number 897] loss: 0.008\n",
      "[epoch 2, pattern number 2389] loss: 0.004\n",
      "[epoch 2, pattern number 3726] loss: 0.008\n",
      "[epoch 2, pattern number 1484] loss: 0.011\n",
      "[epoch 2, pattern number 863] loss: 0.004\n",
      "[epoch 2, pattern number 3287] loss: 0.005\n",
      "[epoch 2, pattern number 2875] loss: 0.007\n",
      "[epoch 2, pattern number 2240] loss: 0.007\n",
      "[epoch 2, pattern number 502] loss: 0.010\n",
      "[epoch 2, pattern number 552] loss: 0.006\n",
      "[epoch 2, pattern number 2258] loss: 0.005\n",
      "[epoch 2, pattern number 1836] loss: 0.007\n",
      "[epoch 2, pattern number 1853] loss: 0.005\n",
      "[epoch 2, pattern number 3157] loss: 0.007\n",
      "[epoch 2, pattern number 3925] loss: 0.005\n",
      "[epoch 2, pattern number 719] loss: 0.007\n",
      "[epoch 2, pattern number 2377] loss: 0.004\n",
      "[epoch 2, pattern number 2789] loss: 0.005\n",
      "[epoch 2, pattern number 1169] loss: 0.006\n",
      "[epoch 2, pattern number 947] loss: 0.009\n",
      "[epoch 2, pattern number 2494] loss: 0.005\n",
      "[epoch 2, pattern number 1376] loss: 0.005\n",
      "[epoch 2, pattern number 924] loss: 0.004\n",
      "[epoch 2, pattern number 3207] loss: 0.006\n",
      "[epoch 2, pattern number 3950] loss: 0.008\n",
      "[epoch 2, pattern number 2991] loss: 0.008\n",
      "[epoch 2, pattern number 1481] loss: 0.008\n",
      "[epoch 2, pattern number 3266] loss: 0.005\n",
      "[epoch 2, pattern number 224] loss: 0.005\n",
      "[epoch 2, pattern number 3859] loss: 0.009\n",
      "[epoch 2, pattern number 644] loss: 0.004\n",
      "[epoch 2, pattern number 98] loss: 0.006\n",
      "[epoch 2, pattern number 246] loss: 0.008\n",
      "[epoch 2, pattern number 2658] loss: 0.007\n",
      "[epoch 2, pattern number 900] loss: 0.005\n",
      "[epoch 2, pattern number 1597] loss: 0.014\n",
      "[epoch 2, pattern number 2483] loss: 0.004\n",
      "[epoch 2, pattern number 2808] loss: 0.007\n",
      "[epoch 2, pattern number 2935] loss: 0.008\n",
      "[epoch 2, pattern number 323] loss: 0.005\n",
      "[epoch 2, pattern number 2549] loss: 0.004\n",
      "[epoch 2, pattern number 3981] loss: 0.006\n",
      "[epoch 2, pattern number 2023] loss: 0.011\n",
      "[epoch 2, pattern number 2162] loss: 0.004\n",
      "[epoch 2, pattern number 473] loss: 0.006\n",
      "[epoch 2, pattern number 248] loss: 0.006\n",
      "[epoch 2, pattern number 3197] loss: 0.007\n",
      "[epoch 2, pattern number 1561] loss: 0.008\n",
      "[epoch 2, pattern number 2651] loss: 0.007\n",
      "[epoch 2, pattern number 761] loss: 0.007\n",
      "[epoch 2, pattern number 3075] loss: 0.006\n",
      "[epoch 2, pattern number 3729] loss: 0.005\n",
      "[epoch 2, pattern number 3935] loss: 0.009\n",
      "[epoch 2, pattern number 3838] loss: 0.005\n",
      "[epoch 2, pattern number 1824] loss: 0.004\n",
      "[epoch 2, pattern number 847] loss: 0.005\n",
      "[epoch 2, pattern number 1709] loss: 0.009\n",
      "[epoch 2, pattern number 2593] loss: 0.006\n",
      "[epoch 2, pattern number 99] loss: 0.007\n",
      "[epoch 2, pattern number 1092] loss: 0.010\n",
      "[epoch 2, pattern number 2061] loss: 0.007\n",
      "[epoch 2, pattern number 1472] loss: 0.006\n",
      "[epoch 2, pattern number 2624] loss: 0.004\n",
      "[epoch 2, pattern number 1221] loss: 0.006\n",
      "[epoch 2, pattern number 3608] loss: 0.010\n",
      "[epoch 2, pattern number 3369] loss: 0.007\n",
      "[epoch 2, pattern number 2610] loss: 0.009\n",
      "[epoch 2, pattern number 2752] loss: 0.006\n",
      "[epoch 2, pattern number 2409] loss: 0.006\n",
      "[epoch 2, pattern number 1278] loss: 0.005\n",
      "[epoch 2, pattern number 2907] loss: 0.009\n",
      "[epoch 2, pattern number 252] loss: 0.006\n",
      "[epoch 2, pattern number 2391] loss: 0.003\n",
      "[epoch 2, pattern number 1674] loss: 0.006\n",
      "[epoch 2, pattern number 1371] loss: 0.004\n",
      "[epoch 2, pattern number 2704] loss: 0.008\n",
      "[epoch 2, pattern number 1162] loss: 0.003\n",
      "[epoch 2, pattern number 1995] loss: 0.006\n",
      "[epoch 2, pattern number 1893] loss: 0.003\n",
      "[epoch 2, pattern number 157] loss: 0.007\n",
      "[epoch 2, pattern number 1943] loss: 0.003\n",
      "[epoch 2, pattern number 3288] loss: 0.007\n",
      "[epoch 2, pattern number 1117] loss: 0.006\n",
      "[epoch 2, pattern number 744] loss: 0.005\n",
      "[epoch 2, pattern number 2923] loss: 0.005\n",
      "[epoch 2, pattern number 291] loss: 0.005\n",
      "[epoch 2, pattern number 2028] loss: 0.005\n",
      "[epoch 2, pattern number 3967] loss: 0.008\n",
      "[epoch 2, pattern number 2126] loss: 0.006\n",
      "[epoch 2, pattern number 3201] loss: 0.007\n",
      "[epoch 2, pattern number 3548] loss: 0.009\n",
      "[epoch 2, pattern number 1837] loss: 0.004\n",
      "[epoch 2, pattern number 240] loss: 0.006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 2, pattern number 3959] loss: 0.004\n",
      "[epoch 2, pattern number 1186] loss: 0.009\n",
      "[epoch 2, pattern number 2423] loss: 0.010\n",
      "[epoch 2, pattern number 3691] loss: 0.004\n",
      "[epoch 2, pattern number 559] loss: 0.007\n",
      "[epoch 2, pattern number 2447] loss: 0.006\n",
      "[epoch 2, pattern number 3200] loss: 0.006\n",
      "[epoch 2, pattern number 1691] loss: 0.007\n",
      "[epoch 2, pattern number 401] loss: 0.007\n",
      "[epoch 2, pattern number 1726] loss: 0.006\n",
      "[epoch 2, pattern number 3278] loss: 0.004\n",
      "[epoch 2, pattern number 3738] loss: 0.008\n",
      "Finished Training\n",
      "Validating in fold number: 2\n",
      "[fold number 2, pattern number 2239] current (single pattern) loss: 0.019\n",
      "[fold number 2, pattern number 3619] current (single pattern) loss: 0.000\n",
      "[fold number 2, pattern number 1699] current (single pattern) loss: 0.000\n",
      "[fold number 2, pattern number 2459] current (single pattern) loss: 0.023\n",
      "[fold number 2, pattern number 1059] current (single pattern) loss: 0.000\n",
      "[fold number 2, pattern number 1519] current (single pattern) loss: 0.009\n",
      "[fold number 2, pattern number 3399] current (single pattern) loss: 0.006\n",
      "[fold number 2, pattern number 3599] current (single pattern) loss: 0.007\n",
      "[fold number 2, pattern number 2939] current (single pattern) loss: 0.001\n",
      "[fold number 2, pattern number 59] current (single pattern) loss: 0.001\n",
      "[fold number 2, pattern number 859] current (single pattern) loss: 0.017\n",
      "[fold number 2, pattern number 259] current (single pattern) loss: 0.003\n",
      "[fold number 2, pattern number 2159] current (single pattern) loss: 0.020\n",
      "[fold number 2, pattern number 3839] current (single pattern) loss: 0.018\n",
      "[fold number 2, pattern number 159] current (single pattern) loss: 0.003\n",
      "[fold number 2, pattern number 3719] current (single pattern) loss: 0.010\n",
      "[fold number 2, pattern number 2579] current (single pattern) loss: 0.003\n",
      "[fold number 2, pattern number 939] current (single pattern) loss: 0.000\n",
      "[fold number 2, pattern number 1439] current (single pattern) loss: 0.020\n",
      "[fold number 2, pattern number 179] current (single pattern) loss: 0.002\n",
      "[fold number 2, pattern number 739] current (single pattern) loss: 0.006\n",
      "[fold number 2, pattern number 1039] current (single pattern) loss: 0.003\n",
      "[fold number 2, pattern number 1259] current (single pattern) loss: 0.003\n",
      "[fold number 2, pattern number 3099] current (single pattern) loss: 0.000\n",
      "[fold number 2, pattern number 2879] current (single pattern) loss: 0.000\n",
      "[fold number 2, pattern number 3079] current (single pattern) loss: 0.002\n",
      "[fold number 2, pattern number 1859] current (single pattern) loss: 0.008\n",
      "[fold number 2, pattern number 2199] current (single pattern) loss: 0.006\n",
      "[fold number 2, pattern number 3739] current (single pattern) loss: 0.006\n",
      "[fold number 2, pattern number 3259] current (single pattern) loss: 0.001\n",
      "[fold number 2, pattern number 1739] current (single pattern) loss: 0.025\n",
      "[fold number 2, pattern number 19] current (single pattern) loss: 0.001\n",
      "[fold number 2, pattern number 2759] current (single pattern) loss: 0.013\n",
      "[fold number 2, pattern number 3539] current (single pattern) loss: 0.016\n",
      "[fold number 2, pattern number 3299] current (single pattern) loss: 0.000\n",
      "[fold number 2, pattern number 2619] current (single pattern) loss: 0.001\n",
      "[fold number 2, pattern number 2659] current (single pattern) loss: 0.002\n",
      "[fold number 2, pattern number 1159] current (single pattern) loss: 0.011\n",
      "[fold number 2, pattern number 1099] current (single pattern) loss: 0.005\n",
      "[fold number 2, pattern number 1479] current (single pattern) loss: 0.003\n",
      "[fold number 2, pattern number 3419] current (single pattern) loss: 0.002\n",
      "[fold number 2, pattern number 2419] current (single pattern) loss: 0.000\n",
      "[fold number 2, pattern number 1239] current (single pattern) loss: 0.015\n",
      "[fold number 2, pattern number 3919] current (single pattern) loss: 0.011\n",
      "[fold number 2, pattern number 2299] current (single pattern) loss: 0.025\n",
      "[fold number 2, pattern number 1279] current (single pattern) loss: 0.020\n",
      "[fold number 2, pattern number 2959] current (single pattern) loss: 0.000\n",
      "[fold number 2, pattern number 2379] current (single pattern) loss: 0.016\n",
      "[fold number 2, pattern number 3499] current (single pattern) loss: 0.000\n",
      "[fold number 2, pattern number 699] current (single pattern) loss: 0.001\n",
      "[fold number 2, pattern number 1359] current (single pattern) loss: 0.003\n",
      "[fold number 2, pattern number 1079] current (single pattern) loss: 0.004\n",
      "[fold number 2, pattern number 979] current (single pattern) loss: 0.000\n",
      "[fold number 2, pattern number 3459] current (single pattern) loss: 0.001\n",
      "[fold number 2, pattern number 3359] current (single pattern) loss: 0.019\n",
      "[fold number 2, pattern number 459] current (single pattern) loss: 0.011\n",
      "[fold number 2, pattern number 799] current (single pattern) loss: 0.008\n",
      "[fold number 2, pattern number 3999] current (single pattern) loss: 0.005\n",
      "[fold number 2, pattern number 3179] current (single pattern) loss: 0.004\n",
      "[fold number 2, pattern number 299] current (single pattern) loss: 0.010\n",
      "[fold number 2, pattern number 519] current (single pattern) loss: 0.002\n",
      "[fold number 2, pattern number 1139] current (single pattern) loss: 0.000\n",
      "[fold number 2, pattern number 2179] current (single pattern) loss: 0.000\n",
      "[fold number 2, pattern number 3759] current (single pattern) loss: 0.001\n",
      "[fold number 2, pattern number 3699] current (single pattern) loss: 0.000\n",
      "[fold number 2, pattern number 999] current (single pattern) loss: 0.000\n",
      "[fold number 2, pattern number 2439] current (single pattern) loss: 0.013\n",
      "[fold number 2, pattern number 2259] current (single pattern) loss: 0.019\n",
      "[fold number 2, pattern number 219] current (single pattern) loss: 0.003\n",
      "[fold number 2, pattern number 2499] current (single pattern) loss: 0.005\n",
      "[fold number 2, pattern number 2779] current (single pattern) loss: 0.005\n",
      "[fold number 2, pattern number 1299] current (single pattern) loss: 0.016\n",
      "[fold number 2, pattern number 959] current (single pattern) loss: 0.010\n",
      "Training in fold number: 3\n",
      "[epoch 1, pattern number 34] loss: 0.484\n",
      "[epoch 1, pattern number 1904] loss: 0.064\n",
      "[epoch 1, pattern number 651] loss: 0.085\n",
      "[epoch 1, pattern number 3127] loss: 0.085\n",
      "[epoch 1, pattern number 1400] loss: 0.027\n",
      "[epoch 1, pattern number 3692] loss: 0.033\n",
      "[epoch 1, pattern number 3617] loss: 0.020\n",
      "[epoch 1, pattern number 1685] loss: 0.023\n",
      "[epoch 1, pattern number 3765] loss: 0.055\n",
      "[epoch 1, pattern number 2894] loss: 0.011\n",
      "[epoch 1, pattern number 1048] loss: 0.023\n",
      "[epoch 1, pattern number 2188] loss: 0.013\n",
      "[epoch 1, pattern number 3922] loss: 0.019\n",
      "[epoch 1, pattern number 1730] loss: 0.009\n",
      "[epoch 1, pattern number 2406] loss: 0.021\n",
      "[epoch 1, pattern number 3194] loss: 0.007\n",
      "[epoch 1, pattern number 833] loss: 0.009\n",
      "[epoch 1, pattern number 3092] loss: 0.006\n",
      "[epoch 1, pattern number 412] loss: 0.013\n",
      "[epoch 1, pattern number 3212] loss: 0.006\n",
      "[epoch 1, pattern number 2943] loss: 0.008\n",
      "[epoch 1, pattern number 3805] loss: 0.009\n",
      "[epoch 1, pattern number 1890] loss: 0.009\n",
      "[epoch 1, pattern number 783] loss: 0.007\n",
      "[epoch 1, pattern number 2588] loss: 0.005\n",
      "[epoch 1, pattern number 1541] loss: 0.016\n",
      "[epoch 1, pattern number 2399] loss: 0.008\n",
      "[epoch 1, pattern number 1857] loss: 0.009\n",
      "[epoch 1, pattern number 423] loss: 0.009\n",
      "[epoch 1, pattern number 3577] loss: 0.010\n",
      "[epoch 1, pattern number 368] loss: 0.006\n",
      "[epoch 1, pattern number 3818] loss: 0.009\n",
      "[epoch 1, pattern number 2192] loss: 0.009\n",
      "[epoch 1, pattern number 1729] loss: 0.010\n",
      "[epoch 1, pattern number 2795] loss: 0.005\n",
      "[epoch 1, pattern number 489] loss: 0.009\n",
      "[epoch 1, pattern number 750] loss: 0.008\n",
      "[epoch 1, pattern number 3056] loss: 0.007\n",
      "[epoch 1, pattern number 3767] loss: 0.009\n",
      "[epoch 1, pattern number 864] loss: 0.007\n",
      "[epoch 1, pattern number 603] loss: 0.013\n",
      "[epoch 1, pattern number 664] loss: 0.006\n",
      "[epoch 1, pattern number 233] loss: 0.009\n",
      "[epoch 1, pattern number 3569] loss: 0.006\n",
      "[epoch 1, pattern number 2663] loss: 0.012\n",
      "[epoch 1, pattern number 1131] loss: 0.006\n",
      "[epoch 1, pattern number 1901] loss: 0.006\n",
      "[epoch 1, pattern number 2459] loss: 0.010\n",
      "[epoch 1, pattern number 768] loss: 0.008\n",
      "[epoch 1, pattern number 1030] loss: 0.022\n",
      "[epoch 1, pattern number 3352] loss: 0.006\n",
      "[epoch 1, pattern number 3963] loss: 0.005\n",
      "[epoch 1, pattern number 3458] loss: 0.007\n",
      "[epoch 1, pattern number 569] loss: 0.007\n",
      "[epoch 1, pattern number 2115] loss: 0.008\n",
      "[epoch 1, pattern number 3968] loss: 0.011\n",
      "[epoch 1, pattern number 255] loss: 0.007\n",
      "[epoch 1, pattern number 3527] loss: 0.010\n",
      "[epoch 1, pattern number 570] loss: 0.010\n",
      "[epoch 1, pattern number 3488] loss: 0.009\n",
      "[epoch 1, pattern number 565] loss: 0.017\n",
      "[epoch 1, pattern number 2186] loss: 0.013\n",
      "[epoch 1, pattern number 878] loss: 0.010\n",
      "[epoch 1, pattern number 848] loss: 0.006\n",
      "[epoch 1, pattern number 2382] loss: 0.008\n",
      "[epoch 1, pattern number 1214] loss: 0.007\n",
      "[epoch 1, pattern number 3594] loss: 0.004\n",
      "[epoch 1, pattern number 2817] loss: 0.010\n",
      "[epoch 1, pattern number 1186] loss: 0.011\n",
      "[epoch 1, pattern number 955] loss: 0.005\n",
      "[epoch 1, pattern number 1505] loss: 0.009\n",
      "[epoch 1, pattern number 3983] loss: 0.006\n",
      "[epoch 1, pattern number 3414] loss: 0.011\n",
      "[epoch 1, pattern number 713] loss: 0.006\n",
      "[epoch 1, pattern number 1523] loss: 0.010\n",
      "[epoch 1, pattern number 971] loss: 0.008\n",
      "[epoch 1, pattern number 1690] loss: 0.011\n",
      "[epoch 1, pattern number 802] loss: 0.005\n",
      "[epoch 1, pattern number 2337] loss: 0.009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1, pattern number 2884] loss: 0.005\n",
      "[epoch 1, pattern number 3954] loss: 0.008\n",
      "[epoch 1, pattern number 3209] loss: 0.009\n",
      "[epoch 1, pattern number 928] loss: 0.006\n",
      "[epoch 1, pattern number 1931] loss: 0.007\n",
      "[epoch 1, pattern number 3283] loss: 0.006\n",
      "[epoch 1, pattern number 3854] loss: 0.006\n",
      "[epoch 1, pattern number 3499] loss: 0.007\n",
      "[epoch 1, pattern number 1586] loss: 0.009\n",
      "[epoch 1, pattern number 1489] loss: 0.004\n",
      "[epoch 1, pattern number 3600] loss: 0.008\n",
      "[epoch 1, pattern number 1167] loss: 0.006\n",
      "[epoch 1, pattern number 3086] loss: 0.005\n",
      "[epoch 1, pattern number 2381] loss: 0.018\n",
      "[epoch 1, pattern number 402] loss: 0.010\n",
      "[epoch 1, pattern number 245] loss: 0.004\n",
      "[epoch 1, pattern number 3238] loss: 0.004\n",
      "[epoch 1, pattern number 23] loss: 0.004\n",
      "[epoch 1, pattern number 545] loss: 0.005\n",
      "[epoch 1, pattern number 1073] loss: 0.005\n",
      "[epoch 1, pattern number 1676] loss: 0.006\n",
      "[epoch 1, pattern number 3917] loss: 0.007\n",
      "[epoch 1, pattern number 1093] loss: 0.009\n",
      "[epoch 1, pattern number 444] loss: 0.008\n",
      "[epoch 1, pattern number 2674] loss: 0.014\n",
      "[epoch 1, pattern number 3934] loss: 0.008\n",
      "[epoch 1, pattern number 2095] loss: 0.009\n",
      "[epoch 1, pattern number 431] loss: 0.008\n",
      "[epoch 1, pattern number 43] loss: 0.010\n",
      "[epoch 1, pattern number 2890] loss: 0.007\n",
      "[epoch 1, pattern number 2297] loss: 0.007\n",
      "[epoch 1, pattern number 2331] loss: 0.007\n",
      "[epoch 1, pattern number 2414] loss: 0.006\n",
      "[epoch 1, pattern number 2314] loss: 0.005\n",
      "[epoch 1, pattern number 3412] loss: 0.005\n",
      "[epoch 1, pattern number 1247] loss: 0.010\n",
      "[epoch 1, pattern number 2910] loss: 0.009\n",
      "[epoch 1, pattern number 3098] loss: 0.007\n",
      "[epoch 1, pattern number 1751] loss: 0.010\n",
      "[epoch 1, pattern number 2506] loss: 0.005\n",
      "[epoch 1, pattern number 46] loss: 0.008\n",
      "[epoch 1, pattern number 3158] loss: 0.008\n",
      "[epoch 1, pattern number 3961] loss: 0.006\n",
      "[epoch 1, pattern number 1065] loss: 0.005\n",
      "[epoch 1, pattern number 1569] loss: 0.006\n",
      "[epoch 1, pattern number 2211] loss: 0.007\n",
      "[epoch 1, pattern number 3144] loss: 0.008\n",
      "[epoch 1, pattern number 1511] loss: 0.005\n",
      "[epoch 1, pattern number 1325] loss: 0.007\n",
      "[epoch 1, pattern number 1452] loss: 0.011\n",
      "[epoch 1, pattern number 101] loss: 0.009\n",
      "[epoch 1, pattern number 2692] loss: 0.008\n",
      "[epoch 1, pattern number 3913] loss: 0.011\n",
      "[epoch 1, pattern number 3156] loss: 0.007\n",
      "[epoch 2, pattern number 308] loss: 0.007\n",
      "[epoch 2, pattern number 2092] loss: 0.006\n",
      "[epoch 2, pattern number 1929] loss: 0.008\n",
      "[epoch 2, pattern number 916] loss: 0.006\n",
      "[epoch 2, pattern number 3729] loss: 0.008\n",
      "[epoch 2, pattern number 3459] loss: 0.005\n",
      "[epoch 2, pattern number 2217] loss: 0.009\n",
      "[epoch 2, pattern number 272] loss: 0.009\n",
      "[epoch 2, pattern number 3492] loss: 0.007\n",
      "[epoch 2, pattern number 3416] loss: 0.006\n",
      "[epoch 2, pattern number 924] loss: 0.007\n",
      "[epoch 2, pattern number 3532] loss: 0.007\n",
      "[epoch 2, pattern number 2297] loss: 0.003\n",
      "[epoch 2, pattern number 131] loss: 0.010\n",
      "[epoch 2, pattern number 1431] loss: 0.011\n",
      "[epoch 2, pattern number 3945] loss: 0.016\n",
      "[epoch 2, pattern number 676] loss: 0.005\n",
      "[epoch 2, pattern number 3687] loss: 0.006\n",
      "[epoch 2, pattern number 1806] loss: 0.005\n",
      "[epoch 2, pattern number 3489] loss: 0.009\n",
      "[epoch 2, pattern number 2428] loss: 0.009\n",
      "[epoch 2, pattern number 464] loss: 0.007\n",
      "[epoch 2, pattern number 1289] loss: 0.010\n",
      "[epoch 2, pattern number 3840] loss: 0.006\n",
      "[epoch 2, pattern number 2804] loss: 0.006\n",
      "[epoch 2, pattern number 1827] loss: 0.008\n",
      "[epoch 2, pattern number 3408] loss: 0.008\n",
      "[epoch 2, pattern number 1321] loss: 0.011\n",
      "[epoch 2, pattern number 1907] loss: 0.003\n",
      "[epoch 2, pattern number 903] loss: 0.006\n",
      "[epoch 2, pattern number 26] loss: 0.012\n",
      "[epoch 2, pattern number 1967] loss: 0.009\n",
      "[epoch 2, pattern number 137] loss: 0.005\n",
      "[epoch 2, pattern number 1576] loss: 0.005\n",
      "[epoch 2, pattern number 822] loss: 0.008\n",
      "[epoch 2, pattern number 959] loss: 0.008\n",
      "[epoch 2, pattern number 1765] loss: 0.007\n",
      "[epoch 2, pattern number 3355] loss: 0.007\n",
      "[epoch 2, pattern number 1285] loss: 0.007\n",
      "[epoch 2, pattern number 2258] loss: 0.006\n",
      "[epoch 2, pattern number 3612] loss: 0.008\n",
      "[epoch 2, pattern number 1224] loss: 0.005\n",
      "[epoch 2, pattern number 664] loss: 0.005\n",
      "[epoch 2, pattern number 2279] loss: 0.011\n",
      "[epoch 2, pattern number 1205] loss: 0.006\n",
      "[epoch 2, pattern number 3179] loss: 0.006\n",
      "[epoch 2, pattern number 3670] loss: 0.008\n",
      "[epoch 2, pattern number 1070] loss: 0.006\n",
      "[epoch 2, pattern number 618] loss: 0.007\n",
      "[epoch 2, pattern number 1611] loss: 0.005\n",
      "[epoch 2, pattern number 2063] loss: 0.009\n",
      "[epoch 2, pattern number 49] loss: 0.009\n",
      "[epoch 2, pattern number 295] loss: 0.004\n",
      "[epoch 2, pattern number 3110] loss: 0.006\n",
      "[epoch 2, pattern number 3474] loss: 0.009\n",
      "[epoch 2, pattern number 472] loss: 0.009\n",
      "[epoch 2, pattern number 2376] loss: 0.007\n",
      "[epoch 2, pattern number 3700] loss: 0.005\n",
      "[epoch 2, pattern number 3594] loss: 0.004\n",
      "[epoch 2, pattern number 3131] loss: 0.005\n",
      "[epoch 2, pattern number 506] loss: 0.009\n",
      "[epoch 2, pattern number 815] loss: 0.009\n",
      "[epoch 2, pattern number 3375] loss: 0.008\n",
      "[epoch 2, pattern number 2948] loss: 0.006\n",
      "[epoch 2, pattern number 303] loss: 0.007\n",
      "[epoch 2, pattern number 707] loss: 0.010\n",
      "[epoch 2, pattern number 2193] loss: 0.005\n",
      "[epoch 2, pattern number 3913] loss: 0.010\n",
      "[epoch 2, pattern number 1143] loss: 0.007\n",
      "[epoch 2, pattern number 2309] loss: 0.005\n",
      "[epoch 2, pattern number 978] loss: 0.009\n",
      "[epoch 2, pattern number 2138] loss: 0.009\n",
      "[epoch 2, pattern number 572] loss: 0.009\n",
      "[epoch 2, pattern number 857] loss: 0.008\n",
      "[epoch 2, pattern number 1412] loss: 0.007\n",
      "[epoch 2, pattern number 192] loss: 0.005\n",
      "[epoch 2, pattern number 2734] loss: 0.004\n",
      "[epoch 2, pattern number 2667] loss: 0.005\n",
      "[epoch 2, pattern number 3398] loss: 0.006\n",
      "[epoch 2, pattern number 2835] loss: 0.009\n",
      "[epoch 2, pattern number 2968] loss: 0.004\n",
      "[epoch 2, pattern number 2270] loss: 0.007\n",
      "[epoch 2, pattern number 1394] loss: 0.008\n",
      "[epoch 2, pattern number 1938] loss: 0.005\n",
      "[epoch 2, pattern number 3589] loss: 0.002\n",
      "[epoch 2, pattern number 799] loss: 0.006\n",
      "[epoch 2, pattern number 954] loss: 0.010\n",
      "[epoch 2, pattern number 1623] loss: 0.005\n",
      "[epoch 2, pattern number 751] loss: 0.008\n",
      "[epoch 2, pattern number 596] loss: 0.008\n",
      "[epoch 2, pattern number 3282] loss: 0.006\n",
      "[epoch 2, pattern number 3357] loss: 0.004\n",
      "[epoch 2, pattern number 3009] loss: 0.007\n",
      "[epoch 2, pattern number 2068] loss: 0.010\n",
      "[epoch 2, pattern number 3380] loss: 0.005\n",
      "[epoch 2, pattern number 2103] loss: 0.009\n",
      "[epoch 2, pattern number 3149] loss: 0.007\n",
      "[epoch 2, pattern number 1835] loss: 0.008\n",
      "[epoch 2, pattern number 824] loss: 0.008\n",
      "[epoch 2, pattern number 1712] loss: 0.009\n",
      "[epoch 2, pattern number 3876] loss: 0.008\n",
      "[epoch 2, pattern number 3907] loss: 0.008\n",
      "[epoch 2, pattern number 3201] loss: 0.011\n",
      "[epoch 2, pattern number 3569] loss: 0.009\n",
      "[epoch 2, pattern number 887] loss: 0.007\n",
      "[epoch 2, pattern number 3087] loss: 0.007\n",
      "[epoch 2, pattern number 3648] loss: 0.004\n",
      "[epoch 2, pattern number 405] loss: 0.005\n",
      "[epoch 2, pattern number 668] loss: 0.012\n",
      "[epoch 2, pattern number 2186] loss: 0.008\n",
      "[epoch 2, pattern number 1278] loss: 0.011\n",
      "[epoch 2, pattern number 1402] loss: 0.006\n",
      "[epoch 2, pattern number 3147] loss: 0.006\n",
      "[epoch 2, pattern number 3251] loss: 0.006\n",
      "[epoch 2, pattern number 2857] loss: 0.007\n",
      "[epoch 2, pattern number 752] loss: 0.009\n",
      "[epoch 2, pattern number 1707] loss: 0.006\n",
      "[epoch 2, pattern number 60] loss: 0.009\n",
      "[epoch 2, pattern number 560] loss: 0.007\n",
      "[epoch 2, pattern number 599] loss: 0.011\n",
      "[epoch 2, pattern number 3671] loss: 0.007\n",
      "[epoch 2, pattern number 619] loss: 0.009\n",
      "[epoch 2, pattern number 2161] loss: 0.009\n",
      "[epoch 2, pattern number 1666] loss: 0.006\n",
      "[epoch 2, pattern number 649] loss: 0.009\n",
      "[epoch 2, pattern number 2490] loss: 0.007\n",
      "[epoch 2, pattern number 160] loss: 0.009\n",
      "[epoch 2, pattern number 2088] loss: 0.008\n",
      "[epoch 2, pattern number 2236] loss: 0.016\n",
      "[epoch 2, pattern number 2959] loss: 0.004\n",
      "[epoch 2, pattern number 2189] loss: 0.008\n",
      "[epoch 2, pattern number 459] loss: 0.008\n",
      "[epoch 2, pattern number 3731] loss: 0.007\n",
      "Finished Training\n",
      "Validating in fold number: 3\n",
      "[fold number 3, pattern number 279] current (single pattern) loss: 0.001\n",
      "[fold number 3, pattern number 1619] current (single pattern) loss: 0.012\n",
      "[fold number 3, pattern number 2039] current (single pattern) loss: 0.015\n",
      "[fold number 3, pattern number 1119] current (single pattern) loss: 0.000\n",
      "[fold number 3, pattern number 1179] current (single pattern) loss: 0.004\n",
      "[fold number 3, pattern number 399] current (single pattern) loss: 0.001\n",
      "[fold number 3, pattern number 2999] current (single pattern) loss: 0.008\n",
      "[fold number 3, pattern number 1959] current (single pattern) loss: 0.004\n",
      "[fold number 3, pattern number 1579] current (single pattern) loss: 0.005\n",
      "[fold number 3, pattern number 559] current (single pattern) loss: 0.007\n",
      "[fold number 3, pattern number 1759] current (single pattern) loss: 0.002\n",
      "[fold number 3, pattern number 3019] current (single pattern) loss: 0.002\n",
      "[fold number 3, pattern number 319] current (single pattern) loss: 0.000\n",
      "[fold number 3, pattern number 3439] current (single pattern) loss: 0.003\n",
      "[fold number 3, pattern number 1399] current (single pattern) loss: 0.000\n",
      "[fold number 3, pattern number 3939] current (single pattern) loss: 0.001\n",
      "[fold number 3, pattern number 199] current (single pattern) loss: 0.010\n",
      "[fold number 3, pattern number 2719] current (single pattern) loss: 0.001\n",
      "[fold number 3, pattern number 1599] current (single pattern) loss: 0.005\n",
      "[fold number 3, pattern number 239] current (single pattern) loss: 0.012\n",
      "[fold number 3, pattern number 2699] current (single pattern) loss: 0.004\n",
      "[fold number 3, pattern number 719] current (single pattern) loss: 0.004\n",
      "[fold number 3, pattern number 779] current (single pattern) loss: 0.004\n",
      "[fold number 3, pattern number 359] current (single pattern) loss: 0.002\n",
      "[fold number 3, pattern number 2839] current (single pattern) loss: 0.009\n",
      "[fold number 3, pattern number 2079] current (single pattern) loss: 0.001\n",
      "[fold number 3, pattern number 3219] current (single pattern) loss: 0.003\n",
      "[fold number 3, pattern number 3039] current (single pattern) loss: 0.006\n",
      "[fold number 3, pattern number 1559] current (single pattern) loss: 0.003\n",
      "[fold number 3, pattern number 3639] current (single pattern) loss: 0.001\n",
      "[fold number 3, pattern number 2679] current (single pattern) loss: 0.013\n",
      "[fold number 3, pattern number 3319] current (single pattern) loss: 0.008\n",
      "[fold number 3, pattern number 3899] current (single pattern) loss: 0.002\n",
      "[fold number 3, pattern number 2899] current (single pattern) loss: 0.010\n",
      "[fold number 3, pattern number 3819] current (single pattern) loss: 0.001\n",
      "[fold number 3, pattern number 1319] current (single pattern) loss: 0.008\n",
      "[fold number 3, pattern number 339] current (single pattern) loss: 0.001\n",
      "[fold number 3, pattern number 3879] current (single pattern) loss: 0.011\n",
      "[fold number 3, pattern number 39] current (single pattern) loss: 0.003\n",
      "[fold number 3, pattern number 3779] current (single pattern) loss: 0.001\n",
      "[fold number 3, pattern number 679] current (single pattern) loss: 0.003\n",
      "[fold number 3, pattern number 2219] current (single pattern) loss: 0.005\n",
      "[fold number 3, pattern number 3479] current (single pattern) loss: 0.007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fold number 3, pattern number 2139] current (single pattern) loss: 0.012\n",
      "[fold number 3, pattern number 1499] current (single pattern) loss: 0.000\n",
      "[fold number 3, pattern number 2359] current (single pattern) loss: 0.000\n",
      "[fold number 3, pattern number 1979] current (single pattern) loss: 0.004\n",
      "[fold number 3, pattern number 2739] current (single pattern) loss: 0.000\n",
      "[fold number 3, pattern number 1779] current (single pattern) loss: 0.000\n",
      "[fold number 3, pattern number 2339] current (single pattern) loss: 0.001\n",
      "[fold number 3, pattern number 479] current (single pattern) loss: 0.011\n",
      "[fold number 3, pattern number 819] current (single pattern) loss: 0.000\n",
      "[fold number 3, pattern number 439] current (single pattern) loss: 0.002\n",
      "[fold number 3, pattern number 99] current (single pattern) loss: 0.003\n",
      "[fold number 3, pattern number 2099] current (single pattern) loss: 0.004\n",
      "[fold number 3, pattern number 379] current (single pattern) loss: 0.000\n",
      "[fold number 3, pattern number 3559] current (single pattern) loss: 0.003\n",
      "[fold number 3, pattern number 759] current (single pattern) loss: 0.001\n",
      "[fold number 3, pattern number 2539] current (single pattern) loss: 0.001\n",
      "[fold number 3, pattern number 3579] current (single pattern) loss: 0.010\n",
      "[fold number 3, pattern number 1539] current (single pattern) loss: 0.002\n",
      "[fold number 3, pattern number 839] current (single pattern) loss: 0.004\n",
      "[fold number 3, pattern number 2639] current (single pattern) loss: 0.002\n",
      "[fold number 3, pattern number 1919] current (single pattern) loss: 0.000\n",
      "[fold number 3, pattern number 2559] current (single pattern) loss: 0.004\n",
      "[fold number 3, pattern number 3239] current (single pattern) loss: 0.018\n",
      "[fold number 3, pattern number 3159] current (single pattern) loss: 0.011\n",
      "Finished fold iterations\n"
     ]
    }
   ],
   "source": [
    "    from IPython.core.display import display, HTML\n",
    "    display(HTML(\"<style>div.output_scroll { height: 44em; }</style>\"))\n",
    "    \n",
    "    fold = 0\n",
    "    loss_function = torch.nn.MSELoss()\n",
    "\n",
    "    for train_index, test_index in kf.split(indices):\n",
    "        fold += 1\n",
    "        print(\"Training in fold number:\", fold)\n",
    "        \n",
    "        # Define the network for this fold. It is a kind of weight reset.\n",
    "        # In more complex scenarios we could use different ANN for every fold.\n",
    "        # For example, assuming there is a function taking an integer and\n",
    "        # returning a network we could make net = get_network_for_fold(fold)\n",
    "        net = Net(n_feature=1, n_hidden=10, n_output=1)\n",
    "        # print(net)  # net architecture\n",
    "    \n",
    "        # We globaly define the hyperparamers but they could be paramerters \n",
    "        # of the training algo.\n",
    "        epochs = 2\n",
    "        optimizer = torch.optim.SGD(net.parameters(), lr=0.2)\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        \n",
    "        current_training_d_without_d_i = SubsetRandomSampler(\n",
    "                indices=train_index)\n",
    "        \n",
    "        current_training_d_loader = torch.utils.data.DataLoader(dataset, \n",
    "                                                            batch_size=1, \n",
    "                                                            shuffle=False, \n",
    "                                        sampler=current_training_d_without_d_i, \n",
    "                                                            batch_sampler=None,\n",
    "                                                            num_workers=0,\n",
    "                                                            collate_fn=None,\n",
    "                                                            pin_memory=False,\n",
    "                                                            drop_last=False,\n",
    "                                                            timeout=0,\n",
    "                                                        worker_init_fn=None, \n",
    "                                                multiprocessing_context=None)\n",
    "        \n",
    "        current_d_i = SubsetRandomSampler(indices=test_index)\n",
    "        \n",
    "        current_d_i_loader = torch.utils.data.DataLoader(dataset, \n",
    "                                                            batch_size=1, \n",
    "                                                            shuffle=False, \n",
    "                                                        sampler=current_d_i, \n",
    "                                                            batch_sampler=None,\n",
    "                                                            num_workers=0,\n",
    "                                                            collate_fn=None,\n",
    "                                                            pin_memory=False,\n",
    "                                                            drop_last=False,\n",
    "                                                            timeout=0,\n",
    "                                                        worker_init_fn=None, \n",
    "                                                multiprocessing_context=None)\n",
    "        # train CNN\n",
    "        # $f_i$ is the learning algorithm. In this case, is the ANN with the \n",
    "        # \"best parameters\" according to the loss function used inside the\n",
    "        # training loop. Note that network architecture, loss function \n",
    "        # (criterion) and number of iterations (epochs) remain constant.\n",
    "        # However, these paramters could be changed to perform a model \n",
    "        # selection/evaluation.\n",
    "        train_ann(ann=net, dataloader=current_training_d_loader, \n",
    "                  criterion=criterion, epochs=epochs)\n",
    "        \n",
    "        f_i = net\n",
    "    \n",
    "        # Calculate loss of the trained model output and the data elements of\n",
    "        # the current partition. Note that we could use now a different loss\n",
    "        # function than the one used to train the network itself. Nevertheless,\n",
    "        # I use the same here (L1 loss).\n",
    "        current_loss = 0.0\n",
    "        print(\"Validating in fold number:\", fold)\n",
    "        i = 0\n",
    "        for data, target, index in current_d_i_loader:\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs = data\n",
    "            labels = target\n",
    "    \n",
    "            # only forward because we are performing evaluation\n",
    "            outputs = net(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "    \n",
    "            # print statistics\n",
    "            current_loss = loss.item()\n",
    "            # index is the pattern/example index\n",
    "            predicted[index] = outputs.item()\n",
    "            error_vector[index] = current_loss\n",
    "            if index % 20 == 19:    # print every 20 examples\n",
    "                print('[fold number %d, pattern number %d] current (single pattern) loss: %.3f' %\n",
    "                      (fold, index, current_loss))\n",
    "            i += 1\n",
    "    print(\"Finished fold iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    }
   ],
   "source": [
    "# Visualize the erros\n",
    "print(error_vector.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import explained_variance_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18669748339461245"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true = dataset.y\n",
    "pred = predicted\n",
    "\n",
    "# Explained variance regression score function.\n",
    "# Best possible score is 1.0, lower values are worse.\n",
    "explained_variance_score(true, pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
