{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "def train_ann(ann=None, dataloader=None, criterion=None, epochs=None):\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        i = 0\n",
    "        for data, target, index in dataloader:\n",
    "            #print(i)\n",
    "            #print(data)\n",
    "            # get the inputs; data is a list of [inputs, labels/target]\n",
    "            inputs = data\n",
    "            labels = target\n",
    "    \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # forward + backward + optimize\n",
    "            outputs = ann(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 20 == 19:    # print every 20 mini-batches\n",
    "                print('[epoch %d, pattern number %d] loss: %.3f' %\n",
    "                      (epoch + 1, index, running_loss / 20))\n",
    "                running_loss = 0.0\n",
    "            i += 1\n",
    "\n",
    "    print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        assert len(X) == len(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        data, target = self.X[i], self.y[i]\n",
    "        # to be able to get the pattern/example index later\n",
    "        index = i\n",
    "        return data,target,index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is one way to define a network\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer\n",
    "        self.predict = torch.nn.Linear(n_hidden, n_output)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x)) # activation function for hidden layer\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':    \n",
    "    torch.manual_seed(1)    # reproducible\n",
    "    \n",
    "    # x data (tensor), shape=(100, 1)\n",
    "    X = torch.unsqueeze(torch.linspace(-1, 1, 4000), dim=1)\n",
    "    # noisy y data (tensor), shape=(100, 1)\n",
    "    y = X.pow(2) + 0.2*torch.rand(X.size())\n",
    "    \n",
    "    dataset = MyDataset(X, y)\n",
    "    \n",
    "    # Set the k-fold\n",
    "    k = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Split the indices into k mutually exclusive subsets $\\mathcal{D}_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "    indices = range(len(dataset))\n",
    "    partitions = kf = KFold(n_splits=k, random_state=None, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error vector contains errors $e_i$ for every pattern $z^{(i)}$.\n",
    "The size of this vector in a sigle task scenario with continuos output\n",
    "(univariate regression) for a dataset with N pattern is (1 x N)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "    error_vector = np.arange(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>div.output_scroll { height: 44em; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in fold number: 1\n",
      "[epoch 1, pattern number 3362] loss: 0.150\n",
      "[epoch 1, pattern number 1147] loss: 0.090\n",
      "[epoch 1, pattern number 3590] loss: 0.094\n",
      "[epoch 1, pattern number 176] loss: 0.073\n",
      "[epoch 1, pattern number 3199] loss: 0.072\n",
      "[epoch 1, pattern number 1679] loss: 0.048\n",
      "[epoch 1, pattern number 1672] loss: 0.009\n",
      "[epoch 1, pattern number 1910] loss: 0.029\n",
      "[epoch 1, pattern number 3858] loss: 0.021\n",
      "[epoch 1, pattern number 2795] loss: 0.012\n",
      "[epoch 1, pattern number 1503] loss: 0.020\n",
      "[epoch 1, pattern number 907] loss: 0.009\n",
      "[epoch 1, pattern number 2722] loss: 0.016\n",
      "[epoch 1, pattern number 3065] loss: 0.017\n",
      "[epoch 1, pattern number 3533] loss: 0.022\n",
      "[epoch 1, pattern number 3664] loss: 0.007\n",
      "[epoch 1, pattern number 2235] loss: 0.004\n",
      "[epoch 1, pattern number 1345] loss: 0.010\n",
      "[epoch 1, pattern number 848] loss: 0.010\n",
      "[epoch 1, pattern number 531] loss: 0.011\n",
      "[epoch 1, pattern number 1605] loss: 0.012\n",
      "[epoch 1, pattern number 1730] loss: 0.009\n",
      "[epoch 1, pattern number 3980] loss: 0.012\n",
      "[epoch 1, pattern number 2889] loss: 0.003\n",
      "[epoch 1, pattern number 1778] loss: 0.011\n",
      "[epoch 1, pattern number 1069] loss: 0.005\n",
      "[epoch 1, pattern number 2507] loss: 0.011\n",
      "[epoch 1, pattern number 971] loss: 0.007\n",
      "[epoch 1, pattern number 2667] loss: 0.011\n",
      "[epoch 1, pattern number 2020] loss: 0.008\n",
      "[epoch 1, pattern number 2304] loss: 0.022\n",
      "[epoch 1, pattern number 254] loss: 0.009\n",
      "[epoch 1, pattern number 3675] loss: 0.013\n",
      "[epoch 1, pattern number 1734] loss: 0.007\n",
      "[epoch 1, pattern number 3463] loss: 0.012\n",
      "[epoch 1, pattern number 2145] loss: 0.007\n",
      "[epoch 1, pattern number 103] loss: 0.007\n",
      "[epoch 1, pattern number 491] loss: 0.008\n",
      "[epoch 1, pattern number 3623] loss: 0.013\n",
      "[epoch 1, pattern number 3070] loss: 0.011\n",
      "[epoch 1, pattern number 704] loss: 0.005\n",
      "[epoch 1, pattern number 1011] loss: 0.008\n",
      "[epoch 1, pattern number 1660] loss: 0.010\n",
      "[epoch 1, pattern number 3385] loss: 0.013\n",
      "[epoch 1, pattern number 1793] loss: 0.017\n",
      "[epoch 1, pattern number 720] loss: 0.008\n",
      "[epoch 1, pattern number 453] loss: 0.006\n",
      "[epoch 1, pattern number 2656] loss: 0.008\n",
      "[epoch 1, pattern number 2999] loss: 0.003\n",
      "[epoch 1, pattern number 3157] loss: 0.006\n",
      "[epoch 1, pattern number 800] loss: 0.006\n",
      "[epoch 1, pattern number 3226] loss: 0.011\n",
      "[epoch 1, pattern number 1400] loss: 0.006\n",
      "[epoch 1, pattern number 1872] loss: 0.004\n",
      "[epoch 1, pattern number 3469] loss: 0.008\n",
      "[epoch 1, pattern number 2569] loss: 0.007\n",
      "[epoch 1, pattern number 74] loss: 0.008\n",
      "[epoch 1, pattern number 182] loss: 0.007\n",
      "[epoch 1, pattern number 935] loss: 0.010\n",
      "[epoch 1, pattern number 1256] loss: 0.005\n",
      "[epoch 1, pattern number 1016] loss: 0.006\n",
      "[epoch 1, pattern number 3160] loss: 0.006\n",
      "[epoch 1, pattern number 3314] loss: 0.009\n",
      "[epoch 1, pattern number 2895] loss: 0.004\n",
      "[epoch 1, pattern number 3762] loss: 0.011\n",
      "[epoch 1, pattern number 1972] loss: 0.003\n",
      "[epoch 1, pattern number 608] loss: 0.006\n",
      "[epoch 1, pattern number 3399] loss: 0.008\n",
      "[epoch 1, pattern number 2686] loss: 0.009\n",
      "[epoch 1, pattern number 197] loss: 0.008\n",
      "[epoch 1, pattern number 714] loss: 0.004\n",
      "[epoch 1, pattern number 374] loss: 0.009\n",
      "[epoch 1, pattern number 2892] loss: 0.010\n",
      "[epoch 1, pattern number 2185] loss: 0.006\n",
      "[epoch 1, pattern number 3414] loss: 0.010\n",
      "[epoch 1, pattern number 3816] loss: 0.008\n",
      "[epoch 1, pattern number 775] loss: 0.014\n",
      "[epoch 1, pattern number 3760] loss: 0.010\n",
      "[epoch 1, pattern number 3629] loss: 0.004\n",
      "[epoch 1, pattern number 3379] loss: 0.006\n",
      "[epoch 1, pattern number 464] loss: 0.009\n",
      "[epoch 1, pattern number 1555] loss: 0.010\n",
      "[epoch 1, pattern number 3183] loss: 0.013\n",
      "[epoch 1, pattern number 740] loss: 0.009\n",
      "[epoch 1, pattern number 2312] loss: 0.008\n",
      "[epoch 1, pattern number 3384] loss: 0.010\n",
      "[epoch 1, pattern number 1396] loss: 0.008\n",
      "[epoch 1, pattern number 1976] loss: 0.011\n",
      "[epoch 1, pattern number 2414] loss: 0.009\n",
      "[epoch 1, pattern number 2988] loss: 0.006\n",
      "[epoch 1, pattern number 333] loss: 0.008\n",
      "[epoch 1, pattern number 1718] loss: 0.006\n",
      "[epoch 1, pattern number 3266] loss: 0.008\n",
      "[epoch 1, pattern number 609] loss: 0.006\n",
      "[epoch 1, pattern number 3203] loss: 0.005\n",
      "[epoch 1, pattern number 919] loss: 0.010\n",
      "[epoch 1, pattern number 2628] loss: 0.007\n",
      "[epoch 1, pattern number 489] loss: 0.010\n",
      "[epoch 1, pattern number 1206] loss: 0.006\n",
      "[epoch 1, pattern number 702] loss: 0.006\n",
      "[epoch 1, pattern number 2427] loss: 0.006\n",
      "[epoch 1, pattern number 3402] loss: 0.002\n",
      "[epoch 1, pattern number 3644] loss: 0.004\n",
      "[epoch 1, pattern number 1570] loss: 0.005\n",
      "[epoch 1, pattern number 2504] loss: 0.011\n",
      "[epoch 1, pattern number 1953] loss: 0.006\n",
      "[epoch 1, pattern number 2461] loss: 0.007\n",
      "[epoch 1, pattern number 2392] loss: 0.006\n",
      "[epoch 1, pattern number 1382] loss: 0.006\n",
      "[epoch 1, pattern number 2775] loss: 0.015\n",
      "[epoch 1, pattern number 751] loss: 0.008\n",
      "[epoch 1, pattern number 367] loss: 0.002\n",
      "[epoch 1, pattern number 114] loss: 0.008\n",
      "[epoch 1, pattern number 1094] loss: 0.007\n",
      "[epoch 1, pattern number 1530] loss: 0.010\n",
      "[epoch 1, pattern number 1837] loss: 0.008\n",
      "[epoch 1, pattern number 63] loss: 0.006\n",
      "[epoch 1, pattern number 725] loss: 0.007\n",
      "[epoch 1, pattern number 1829] loss: 0.006\n",
      "[epoch 1, pattern number 3263] loss: 0.004\n",
      "[epoch 1, pattern number 754] loss: 0.006\n",
      "[epoch 1, pattern number 1306] loss: 0.004\n",
      "[epoch 1, pattern number 3117] loss: 0.007\n",
      "[epoch 1, pattern number 850] loss: 0.007\n",
      "[epoch 1, pattern number 2138] loss: 0.004\n",
      "[epoch 1, pattern number 3210] loss: 0.007\n",
      "[epoch 1, pattern number 1514] loss: 0.007\n",
      "[epoch 1, pattern number 2232] loss: 0.005\n",
      "[epoch 1, pattern number 3856] loss: 0.007\n",
      "[epoch 1, pattern number 2748] loss: 0.006\n",
      "[epoch 1, pattern number 3540] loss: 0.008\n",
      "[epoch 1, pattern number 3650] loss: 0.012\n",
      "[epoch 1, pattern number 2816] loss: 0.012\n",
      "[epoch 2, pattern number 1206] loss: 0.005\n",
      "[epoch 2, pattern number 1495] loss: 0.007\n",
      "[epoch 2, pattern number 2976] loss: 0.006\n",
      "[epoch 2, pattern number 560] loss: 0.007\n",
      "[epoch 2, pattern number 2814] loss: 0.009\n",
      "[epoch 2, pattern number 3520] loss: 0.007\n",
      "[epoch 2, pattern number 1658] loss: 0.005\n",
      "[epoch 2, pattern number 2338] loss: 0.012\n",
      "[epoch 2, pattern number 3103] loss: 0.004\n",
      "[epoch 2, pattern number 2412] loss: 0.003\n",
      "[epoch 2, pattern number 138] loss: 0.009\n",
      "[epoch 2, pattern number 2209] loss: 0.009\n",
      "[epoch 2, pattern number 1277] loss: 0.006\n",
      "[epoch 2, pattern number 3922] loss: 0.008\n",
      "[epoch 2, pattern number 3063] loss: 0.008\n",
      "[epoch 2, pattern number 1687] loss: 0.006\n",
      "[epoch 2, pattern number 2455] loss: 0.008\n",
      "[epoch 2, pattern number 1123] loss: 0.012\n",
      "[epoch 2, pattern number 296] loss: 0.005\n",
      "[epoch 2, pattern number 324] loss: 0.008\n",
      "[epoch 2, pattern number 589] loss: 0.004\n",
      "[epoch 2, pattern number 1069] loss: 0.005\n",
      "[epoch 2, pattern number 1158] loss: 0.006\n",
      "[epoch 2, pattern number 2524] loss: 0.006\n",
      "[epoch 2, pattern number 343] loss: 0.005\n",
      "[epoch 2, pattern number 766] loss: 0.006\n",
      "[epoch 2, pattern number 2527] loss: 0.007\n",
      "[epoch 2, pattern number 3023] loss: 0.005\n",
      "[epoch 2, pattern number 91] loss: 0.007\n",
      "[epoch 2, pattern number 2607] loss: 0.010\n",
      "[epoch 2, pattern number 3297] loss: 0.008\n",
      "[epoch 2, pattern number 1910] loss: 0.005\n",
      "[epoch 2, pattern number 1139] loss: 0.008\n",
      "[epoch 2, pattern number 887] loss: 0.009\n",
      "[epoch 2, pattern number 1284] loss: 0.007\n",
      "[epoch 2, pattern number 3477] loss: 0.008\n",
      "[epoch 2, pattern number 1183] loss: 0.007\n",
      "[epoch 2, pattern number 3838] loss: 0.006\n",
      "[epoch 2, pattern number 3314] loss: 0.004\n",
      "[epoch 2, pattern number 3040] loss: 0.010\n",
      "[epoch 2, pattern number 3612] loss: 0.005\n",
      "[epoch 2, pattern number 291] loss: 0.014\n",
      "[epoch 2, pattern number 1825] loss: 0.005\n",
      "[epoch 2, pattern number 1457] loss: 0.006\n",
      "[epoch 2, pattern number 876] loss: 0.007\n",
      "[epoch 2, pattern number 3097] loss: 0.007\n",
      "[epoch 2, pattern number 1941] loss: 0.006\n",
      "[epoch 2, pattern number 2216] loss: 0.005\n",
      "[epoch 2, pattern number 3525] loss: 0.003\n",
      "[epoch 2, pattern number 1677] loss: 0.005\n",
      "[epoch 2, pattern number 1364] loss: 0.008\n",
      "[epoch 2, pattern number 236] loss: 0.007\n",
      "[epoch 2, pattern number 3622] loss: 0.007\n",
      "[epoch 2, pattern number 2369] loss: 0.006\n",
      "[epoch 2, pattern number 907] loss: 0.009\n",
      "[epoch 2, pattern number 1935] loss: 0.003\n",
      "[epoch 2, pattern number 2622] loss: 0.004\n",
      "[epoch 2, pattern number 1797] loss: 0.008\n",
      "[epoch 2, pattern number 1480] loss: 0.008\n",
      "[epoch 2, pattern number 1835] loss: 0.007\n",
      "[epoch 2, pattern number 3357] loss: 0.006\n",
      "[epoch 2, pattern number 3148] loss: 0.006\n",
      "[epoch 2, pattern number 1041] loss: 0.009\n",
      "[epoch 2, pattern number 1451] loss: 0.011\n",
      "[epoch 2, pattern number 1275] loss: 0.006\n",
      "[epoch 2, pattern number 3664] loss: 0.004\n",
      "[epoch 2, pattern number 1841] loss: 0.008\n",
      "[epoch 2, pattern number 620] loss: 0.008\n",
      "[epoch 2, pattern number 1478] loss: 0.011\n",
      "[epoch 2, pattern number 3049] loss: 0.002\n",
      "[epoch 2, pattern number 2580] loss: 0.009\n",
      "[epoch 2, pattern number 1334] loss: 0.004\n",
      "[epoch 2, pattern number 3633] loss: 0.008\n",
      "[epoch 2, pattern number 363] loss: 0.003\n",
      "[epoch 2, pattern number 1969] loss: 0.002\n",
      "[epoch 2, pattern number 412] loss: 0.008\n",
      "[epoch 2, pattern number 3162] loss: 0.004\n",
      "[epoch 2, pattern number 727] loss: 0.005\n",
      "[epoch 2, pattern number 1851] loss: 0.011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 2, pattern number 3986] loss: 0.006\n",
      "[epoch 2, pattern number 2507] loss: 0.011\n",
      "[epoch 2, pattern number 1001] loss: 0.005\n",
      "[epoch 2, pattern number 1987] loss: 0.005\n",
      "[epoch 2, pattern number 916] loss: 0.011\n",
      "[epoch 2, pattern number 3571] loss: 0.010\n",
      "[epoch 2, pattern number 3707] loss: 0.005\n",
      "[epoch 2, pattern number 227] loss: 0.006\n",
      "[epoch 2, pattern number 2406] loss: 0.004\n",
      "[epoch 2, pattern number 927] loss: 0.007\n",
      "[epoch 2, pattern number 3656] loss: 0.005\n",
      "[epoch 2, pattern number 1487] loss: 0.005\n",
      "[epoch 2, pattern number 2162] loss: 0.007\n",
      "[epoch 2, pattern number 3661] loss: 0.008\n",
      "[epoch 2, pattern number 2178] loss: 0.006\n",
      "[epoch 2, pattern number 3137] loss: 0.005\n",
      "[epoch 2, pattern number 1404] loss: 0.012\n",
      "[epoch 2, pattern number 2945] loss: 0.007\n",
      "[epoch 2, pattern number 1023] loss: 0.006\n",
      "[epoch 2, pattern number 711] loss: 0.007\n",
      "[epoch 2, pattern number 2109] loss: 0.011\n",
      "[epoch 2, pattern number 1556] loss: 0.007\n",
      "[epoch 2, pattern number 1824] loss: 0.007\n",
      "[epoch 2, pattern number 827] loss: 0.008\n",
      "[epoch 2, pattern number 1442] loss: 0.015\n",
      "[epoch 2, pattern number 3199] loss: 0.006\n",
      "[epoch 2, pattern number 2123] loss: 0.006\n",
      "[epoch 2, pattern number 3174] loss: 0.006\n",
      "[epoch 2, pattern number 3718] loss: 0.005\n",
      "[epoch 2, pattern number 1609] loss: 0.003\n",
      "[epoch 2, pattern number 336] loss: 0.004\n",
      "[epoch 2, pattern number 2451] loss: 0.006\n",
      "[epoch 2, pattern number 717] loss: 0.009\n",
      "[epoch 2, pattern number 595] loss: 0.007\n",
      "[epoch 2, pattern number 3882] loss: 0.006\n",
      "[epoch 2, pattern number 2741] loss: 0.006\n",
      "[epoch 2, pattern number 96] loss: 0.006\n",
      "[epoch 2, pattern number 3601] loss: 0.007\n",
      "[epoch 2, pattern number 1391] loss: 0.009\n",
      "[epoch 2, pattern number 3130] loss: 0.009\n",
      "[epoch 2, pattern number 656] loss: 0.015\n",
      "[epoch 2, pattern number 3852] loss: 0.005\n",
      "[epoch 2, pattern number 300] loss: 0.005\n",
      "[epoch 2, pattern number 3753] loss: 0.008\n",
      "[epoch 2, pattern number 142] loss: 0.008\n",
      "[epoch 2, pattern number 2315] loss: 0.005\n",
      "[epoch 2, pattern number 39] loss: 0.004\n",
      "[epoch 2, pattern number 3869] loss: 0.007\n",
      "[epoch 2, pattern number 3487] loss: 0.007\n",
      "[epoch 2, pattern number 2879] loss: 0.009\n",
      "[epoch 2, pattern number 2526] loss: 0.005\n",
      "[epoch 2, pattern number 2461] loss: 0.006\n",
      "[epoch 2, pattern number 2641] loss: 0.010\n",
      "[epoch 2, pattern number 406] loss: 0.004\n",
      "Finished Training\n",
      "Validating in fold number: 1\n",
      "[fold number 1, pattern number 3419] current (single pattern) loss: 0.002\n",
      "[fold number 1, pattern number 1639] current (single pattern) loss: 0.004\n",
      "[fold number 1, pattern number 3019] current (single pattern) loss: 0.002\n",
      "[fold number 1, pattern number 2919] current (single pattern) loss: 0.000\n",
      "[fold number 1, pattern number 2119] current (single pattern) loss: 0.020\n",
      "[fold number 1, pattern number 679] current (single pattern) loss: 0.010\n",
      "[fold number 1, pattern number 3919] current (single pattern) loss: 0.005\n",
      "[fold number 1, pattern number 479] current (single pattern) loss: 0.021\n",
      "[fold number 1, pattern number 2619] current (single pattern) loss: 0.001\n",
      "[fold number 1, pattern number 1699] current (single pattern) loss: 0.000\n",
      "[fold number 1, pattern number 3159] current (single pattern) loss: 0.013\n",
      "[fold number 1, pattern number 3179] current (single pattern) loss: 0.002\n",
      "[fold number 1, pattern number 659] current (single pattern) loss: 0.001\n",
      "[fold number 1, pattern number 3099] current (single pattern) loss: 0.000\n",
      "[fold number 1, pattern number 399] current (single pattern) loss: 0.000\n",
      "[fold number 1, pattern number 499] current (single pattern) loss: 0.001\n",
      "[fold number 1, pattern number 3119] current (single pattern) loss: 0.003\n",
      "[fold number 1, pattern number 3459] current (single pattern) loss: 0.001\n",
      "[fold number 1, pattern number 3499] current (single pattern) loss: 0.000\n",
      "[fold number 1, pattern number 1119] current (single pattern) loss: 0.001\n",
      "[fold number 1, pattern number 439] current (single pattern) loss: 0.000\n",
      "[fold number 1, pattern number 19] current (single pattern) loss: 0.000\n",
      "[fold number 1, pattern number 1999] current (single pattern) loss: 0.014\n",
      "[fold number 1, pattern number 3639] current (single pattern) loss: 0.003\n",
      "[fold number 1, pattern number 719] current (single pattern) loss: 0.011\n",
      "[fold number 1, pattern number 2699] current (single pattern) loss: 0.006\n",
      "[fold number 1, pattern number 1919] current (single pattern) loss: 0.002\n",
      "[fold number 1, pattern number 2499] current (single pattern) loss: 0.001\n",
      "[fold number 1, pattern number 2359] current (single pattern) loss: 0.001\n",
      "[fold number 1, pattern number 639] current (single pattern) loss: 0.014\n",
      "[fold number 1, pattern number 99] current (single pattern) loss: 0.007\n",
      "[fold number 1, pattern number 2019] current (single pattern) loss: 0.026\n",
      "[fold number 1, pattern number 1319] current (single pattern) loss: 0.018\n",
      "[fold number 1, pattern number 2139] current (single pattern) loss: 0.018\n",
      "[fold number 1, pattern number 1239] current (single pattern) loss: 0.007\n",
      "[fold number 1, pattern number 2539] current (single pattern) loss: 0.002\n",
      "[fold number 1, pattern number 1219] current (single pattern) loss: 0.015\n",
      "[fold number 1, pattern number 3679] current (single pattern) loss: 0.001\n",
      "[fold number 1, pattern number 299] current (single pattern) loss: 0.017\n",
      "[fold number 1, pattern number 3579] current (single pattern) loss: 0.016\n",
      "[fold number 1, pattern number 2939] current (single pattern) loss: 0.000\n",
      "[fold number 1, pattern number 3539] current (single pattern) loss: 0.014\n",
      "[fold number 1, pattern number 759] current (single pattern) loss: 0.005\n",
      "[fold number 1, pattern number 2779] current (single pattern) loss: 0.000\n",
      "[fold number 1, pattern number 899] current (single pattern) loss: 0.003\n",
      "[fold number 1, pattern number 79] current (single pattern) loss: 0.000\n",
      "[fold number 1, pattern number 3519] current (single pattern) loss: 0.000\n",
      "[fold number 1, pattern number 2179] current (single pattern) loss: 0.000\n",
      "[fold number 1, pattern number 2379] current (single pattern) loss: 0.008\n",
      "[fold number 1, pattern number 559] current (single pattern) loss: 0.017\n",
      "[fold number 1, pattern number 1439] current (single pattern) loss: 0.013\n",
      "[fold number 1, pattern number 979] current (single pattern) loss: 0.000\n",
      "[fold number 1, pattern number 3859] current (single pattern) loss: 0.001\n",
      "[fold number 1, pattern number 2519] current (single pattern) loss: 0.002\n",
      "[fold number 1, pattern number 1199] current (single pattern) loss: 0.000\n",
      "[fold number 1, pattern number 2159] current (single pattern) loss: 0.014\n",
      "[fold number 1, pattern number 799] current (single pattern) loss: 0.013\n",
      "[fold number 1, pattern number 1879] current (single pattern) loss: 0.018\n",
      "[fold number 1, pattern number 1979] current (single pattern) loss: 0.009\n",
      "[fold number 1, pattern number 459] current (single pattern) loss: 0.016\n",
      "[fold number 1, pattern number 1799] current (single pattern) loss: 0.004\n",
      "[fold number 1, pattern number 1179] current (single pattern) loss: 0.012\n",
      "[fold number 1, pattern number 2439] current (single pattern) loss: 0.005\n",
      "[fold number 1, pattern number 1779] current (single pattern) loss: 0.001\n",
      "[fold number 1, pattern number 1419] current (single pattern) loss: 0.016\n",
      "[fold number 1, pattern number 3739] current (single pattern) loss: 0.003\n",
      "[fold number 1, pattern number 1539] current (single pattern) loss: 0.000\n",
      "[fold number 1, pattern number 2079] current (single pattern) loss: 0.003\n",
      "[fold number 1, pattern number 2039] current (single pattern) loss: 0.023\n",
      "[fold number 1, pattern number 2599] current (single pattern) loss: 0.000\n",
      "[fold number 1, pattern number 199] current (single pattern) loss: 0.019\n",
      "Training in fold number: 2\n",
      "[epoch 1, pattern number 2294] loss: 0.158\n",
      "[epoch 1, pattern number 3467] loss: 0.145\n",
      "[epoch 1, pattern number 3042] loss: 0.034\n",
      "[epoch 1, pattern number 1783] loss: 0.009\n",
      "[epoch 1, pattern number 2240] loss: 0.008\n",
      "[epoch 1, pattern number 3442] loss: 0.013\n",
      "[epoch 1, pattern number 1835] loss: 0.030\n",
      "[epoch 1, pattern number 2238] loss: 0.015\n",
      "[epoch 1, pattern number 298] loss: 0.029\n",
      "[epoch 1, pattern number 2786] loss: 0.009\n",
      "[epoch 1, pattern number 3690] loss: 0.018\n",
      "[epoch 1, pattern number 1639] loss: 0.009\n",
      "[epoch 1, pattern number 3771] loss: 0.008\n",
      "[epoch 1, pattern number 2579] loss: 0.012\n",
      "[epoch 1, pattern number 3730] loss: 0.012\n",
      "[epoch 1, pattern number 1086] loss: 0.009\n",
      "[epoch 1, pattern number 1110] loss: 0.017\n",
      "[epoch 1, pattern number 2224] loss: 0.003\n",
      "[epoch 1, pattern number 2867] loss: 0.010\n",
      "[epoch 1, pattern number 1905] loss: 0.010\n",
      "[epoch 1, pattern number 820] loss: 0.009\n",
      "[epoch 1, pattern number 3400] loss: 0.008\n",
      "[epoch 1, pattern number 499] loss: 0.010\n",
      "[epoch 1, pattern number 3645] loss: 0.004\n",
      "[epoch 1, pattern number 3428] loss: 0.007\n",
      "[epoch 1, pattern number 2040] loss: 0.007\n",
      "[epoch 1, pattern number 69] loss: 0.010\n",
      "[epoch 1, pattern number 468] loss: 0.016\n",
      "[epoch 1, pattern number 2126] loss: 0.007\n",
      "[epoch 1, pattern number 1676] loss: 0.004\n",
      "[epoch 1, pattern number 445] loss: 0.008\n",
      "[epoch 1, pattern number 2752] loss: 0.010\n",
      "[epoch 1, pattern number 1179] loss: 0.008\n",
      "[epoch 1, pattern number 1582] loss: 0.005\n",
      "[epoch 1, pattern number 3472] loss: 0.008\n",
      "[epoch 1, pattern number 2159] loss: 0.009\n",
      "[epoch 1, pattern number 2434] loss: 0.003\n",
      "[epoch 1, pattern number 3257] loss: 0.005\n",
      "[epoch 1, pattern number 1636] loss: 0.007\n",
      "[epoch 1, pattern number 3393] loss: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1, pattern number 3038] loss: 0.005\n",
      "[epoch 1, pattern number 2674] loss: 0.006\n",
      "[epoch 1, pattern number 2749] loss: 0.006\n",
      "[epoch 1, pattern number 900] loss: 0.003\n",
      "[epoch 1, pattern number 2808] loss: 0.010\n",
      "[epoch 1, pattern number 2739] loss: 0.004\n",
      "[epoch 1, pattern number 566] loss: 0.006\n",
      "[epoch 1, pattern number 3837] loss: 0.006\n",
      "[epoch 1, pattern number 3162] loss: 0.006\n",
      "[epoch 1, pattern number 545] loss: 0.006\n",
      "[epoch 1, pattern number 3228] loss: 0.006\n",
      "[epoch 1, pattern number 1316] loss: 0.004\n",
      "[epoch 1, pattern number 207] loss: 0.007\n",
      "[epoch 1, pattern number 3744] loss: 0.006\n",
      "[epoch 1, pattern number 2886] loss: 0.006\n",
      "[epoch 1, pattern number 1237] loss: 0.005\n",
      "[epoch 1, pattern number 934] loss: 0.007\n",
      "[epoch 1, pattern number 970] loss: 0.007\n",
      "[epoch 1, pattern number 1858] loss: 0.007\n",
      "[epoch 1, pattern number 3426] loss: 0.007\n",
      "[epoch 1, pattern number 3762] loss: 0.006\n",
      "[epoch 1, pattern number 1736] loss: 0.006\n",
      "[epoch 1, pattern number 3406] loss: 0.004\n",
      "[epoch 1, pattern number 804] loss: 0.009\n",
      "[epoch 1, pattern number 2765] loss: 0.008\n",
      "[epoch 1, pattern number 3542] loss: 0.008\n",
      "[epoch 1, pattern number 1494] loss: 0.008\n",
      "[epoch 1, pattern number 2572] loss: 0.004\n",
      "[epoch 1, pattern number 1334] loss: 0.006\n",
      "[epoch 1, pattern number 2477] loss: 0.008\n",
      "[epoch 1, pattern number 219] loss: 0.007\n",
      "[epoch 1, pattern number 449] loss: 0.009\n",
      "[epoch 1, pattern number 1236] loss: 0.011\n",
      "[epoch 1, pattern number 2975] loss: 0.004\n",
      "[epoch 1, pattern number 3917] loss: 0.011\n",
      "[epoch 1, pattern number 3157] loss: 0.010\n",
      "[epoch 1, pattern number 768] loss: 0.009\n",
      "[epoch 1, pattern number 3027] loss: 0.008\n",
      "[epoch 1, pattern number 2367] loss: 0.012\n",
      "[epoch 1, pattern number 509] loss: 0.006\n",
      "[epoch 1, pattern number 3243] loss: 0.009\n",
      "[epoch 1, pattern number 1009] loss: 0.004\n",
      "[epoch 1, pattern number 552] loss: 0.010\n",
      "[epoch 1, pattern number 1567] loss: 0.008\n",
      "[epoch 1, pattern number 3459] loss: 0.008\n",
      "[epoch 1, pattern number 935] loss: 0.006\n",
      "[epoch 1, pattern number 2014] loss: 0.004\n",
      "[epoch 1, pattern number 111] loss: 0.008\n",
      "[epoch 1, pattern number 3293] loss: 0.005\n",
      "[epoch 1, pattern number 1251] loss: 0.007\n",
      "[epoch 1, pattern number 2384] loss: 0.006\n",
      "[epoch 1, pattern number 3269] loss: 0.012\n",
      "[epoch 1, pattern number 31] loss: 0.004\n",
      "[epoch 1, pattern number 2673] loss: 0.009\n",
      "[epoch 1, pattern number 250] loss: 0.007\n",
      "[epoch 1, pattern number 1365] loss: 0.008\n",
      "[epoch 1, pattern number 3509] loss: 0.004\n",
      "[epoch 1, pattern number 3270] loss: 0.005\n",
      "[epoch 1, pattern number 3117] loss: 0.010\n",
      "[epoch 1, pattern number 1912] loss: 0.004\n",
      "[epoch 1, pattern number 1440] loss: 0.007\n",
      "[epoch 1, pattern number 494] loss: 0.005\n",
      "[epoch 1, pattern number 2692] loss: 0.007\n",
      "[epoch 1, pattern number 3007] loss: 0.006\n",
      "[epoch 1, pattern number 3641] loss: 0.008\n",
      "[epoch 1, pattern number 1201] loss: 0.014\n",
      "[epoch 1, pattern number 128] loss: 0.003\n",
      "[epoch 1, pattern number 2022] loss: 0.005\n",
      "[epoch 1, pattern number 1449] loss: 0.008\n",
      "[epoch 1, pattern number 2292] loss: 0.008\n",
      "[epoch 1, pattern number 802] loss: 0.006\n",
      "[epoch 1, pattern number 3646] loss: 0.010\n",
      "[epoch 1, pattern number 477] loss: 0.011\n",
      "[epoch 1, pattern number 1920] loss: 0.007\n",
      "[epoch 1, pattern number 1022] loss: 0.003\n",
      "[epoch 1, pattern number 3223] loss: 0.004\n",
      "[epoch 1, pattern number 3580] loss: 0.006\n",
      "[epoch 1, pattern number 672] loss: 0.008\n",
      "[epoch 1, pattern number 3904] loss: 0.009\n",
      "[epoch 1, pattern number 1149] loss: 0.004\n",
      "[epoch 1, pattern number 327] loss: 0.008\n",
      "[epoch 1, pattern number 629] loss: 0.004\n",
      "[epoch 1, pattern number 1323] loss: 0.008\n",
      "[epoch 1, pattern number 469] loss: 0.005\n",
      "[epoch 1, pattern number 356] loss: 0.005\n",
      "[epoch 1, pattern number 510] loss: 0.004\n",
      "[epoch 1, pattern number 2699] loss: 0.007\n",
      "[epoch 1, pattern number 2508] loss: 0.004\n",
      "[epoch 1, pattern number 2279] loss: 0.012\n",
      "[epoch 1, pattern number 208] loss: 0.008\n",
      "[epoch 1, pattern number 460] loss: 0.007\n",
      "[epoch 1, pattern number 32] loss: 0.005\n",
      "[epoch 1, pattern number 1552] loss: 0.005\n",
      "[epoch 2, pattern number 192] loss: 0.005\n",
      "[epoch 2, pattern number 1950] loss: 0.007\n",
      "[epoch 2, pattern number 707] loss: 0.005\n",
      "[epoch 2, pattern number 1593] loss: 0.005\n",
      "[epoch 2, pattern number 3473] loss: 0.006\n",
      "[epoch 2, pattern number 206] loss: 0.007\n",
      "[epoch 2, pattern number 2297] loss: 0.004\n",
      "[epoch 2, pattern number 11] loss: 0.012\n",
      "[epoch 2, pattern number 639] loss: 0.009\n",
      "[epoch 2, pattern number 54] loss: 0.012\n",
      "[epoch 2, pattern number 2210] loss: 0.007\n",
      "[epoch 2, pattern number 1829] loss: 0.011\n",
      "[epoch 2, pattern number 2877] loss: 0.005\n",
      "[epoch 2, pattern number 755] loss: 0.007\n",
      "[epoch 2, pattern number 2449] loss: 0.006\n",
      "[epoch 2, pattern number 3121] loss: 0.007\n",
      "[epoch 2, pattern number 306] loss: 0.004\n",
      "[epoch 2, pattern number 444] loss: 0.003\n",
      "[epoch 2, pattern number 1659] loss: 0.003\n",
      "[epoch 2, pattern number 3492] loss: 0.007\n",
      "[epoch 2, pattern number 3516] loss: 0.006\n",
      "[epoch 2, pattern number 2293] loss: 0.004\n",
      "[epoch 2, pattern number 2807] loss: 0.006\n",
      "[epoch 2, pattern number 804] loss: 0.008\n",
      "[epoch 2, pattern number 1780] loss: 0.004\n",
      "[epoch 2, pattern number 3654] loss: 0.005\n",
      "[epoch 2, pattern number 1395] loss: 0.008\n",
      "[epoch 2, pattern number 3796] loss: 0.006\n",
      "[epoch 2, pattern number 903] loss: 0.007\n",
      "[epoch 2, pattern number 2414] loss: 0.004\n",
      "[epoch 2, pattern number 3715] loss: 0.004\n",
      "[epoch 2, pattern number 1533] loss: 0.011\n",
      "[epoch 2, pattern number 862] loss: 0.009\n",
      "[epoch 2, pattern number 3313] loss: 0.007\n",
      "[epoch 2, pattern number 2919] loss: 0.006\n",
      "[epoch 2, pattern number 2260] loss: 0.007\n",
      "[epoch 2, pattern number 496] loss: 0.008\n",
      "[epoch 2, pattern number 551] loss: 0.007\n",
      "[epoch 2, pattern number 2277] loss: 0.005\n",
      "[epoch 2, pattern number 1880] loss: 0.011\n",
      "[epoch 2, pattern number 1894] loss: 0.005\n",
      "[epoch 2, pattern number 3176] loss: 0.004\n",
      "[epoch 2, pattern number 3922] loss: 0.005\n",
      "[epoch 2, pattern number 711] loss: 0.006\n",
      "[epoch 2, pattern number 2394] loss: 0.004\n",
      "[epoch 2, pattern number 2823] loss: 0.006\n",
      "[epoch 2, pattern number 1196] loss: 0.003\n",
      "[epoch 2, pattern number 962] loss: 0.005\n",
      "[epoch 2, pattern number 2515] loss: 0.008\n",
      "[epoch 2, pattern number 1408] loss: 0.005\n",
      "[epoch 2, pattern number 933] loss: 0.007\n",
      "[epoch 2, pattern number 3232] loss: 0.004\n",
      "[epoch 2, pattern number 3947] loss: 0.007\n",
      "[epoch 2, pattern number 3026] loss: 0.005\n",
      "[epoch 2, pattern number 1523] loss: 0.004\n",
      "[epoch 2, pattern number 3298] loss: 0.009\n",
      "[epoch 2, pattern number 204] loss: 0.006\n",
      "[epoch 2, pattern number 3845] loss: 0.007\n",
      "[epoch 2, pattern number 643] loss: 0.003\n",
      "[epoch 2, pattern number 92] loss: 0.006\n",
      "[epoch 2, pattern number 222] loss: 0.012\n",
      "[epoch 2, pattern number 2692] loss: 0.006\n",
      "[epoch 2, pattern number 908] loss: 0.005\n",
      "[epoch 2, pattern number 1651] loss: 0.008\n",
      "[epoch 2, pattern number 2503] loss: 0.006\n",
      "[epoch 2, pattern number 2847] loss: 0.003\n",
      "[epoch 2, pattern number 2972] loss: 0.005\n",
      "[epoch 2, pattern number 299] loss: 0.006\n",
      "[epoch 2, pattern number 2563] loss: 0.003\n",
      "[epoch 2, pattern number 3985] loss: 0.007\n",
      "[epoch 2, pattern number 2052] loss: 0.003\n",
      "[epoch 2, pattern number 2201] loss: 0.006\n",
      "[epoch 2, pattern number 459] loss: 0.008\n",
      "[epoch 2, pattern number 224] loss: 0.008\n",
      "[epoch 2, pattern number 3220] loss: 0.004\n",
      "[epoch 2, pattern number 1607] loss: 0.006\n",
      "[epoch 2, pattern number 2682] loss: 0.005\n",
      "[epoch 2, pattern number 762] loss: 0.009\n",
      "[epoch 2, pattern number 3109] loss: 0.004\n",
      "[epoch 2, pattern number 3720] loss: 0.009\n",
      "[epoch 2, pattern number 3932] loss: 0.006\n",
      "[epoch 2, pattern number 3822] loss: 0.004\n",
      "[epoch 2, pattern number 1861] loss: 0.005\n",
      "[epoch 2, pattern number 849] loss: 0.006\n",
      "[epoch 2, pattern number 1752] loss: 0.007\n",
      "[epoch 2, pattern number 2618] loss: 0.005\n",
      "[epoch 2, pattern number 95] loss: 0.006\n",
      "[epoch 2, pattern number 1117] loss: 0.004\n",
      "[epoch 2, pattern number 2105] loss: 0.008\n",
      "[epoch 2, pattern number 1516] loss: 0.005\n",
      "[epoch 2, pattern number 2652] loss: 0.003\n",
      "[epoch 2, pattern number 1248] loss: 0.005\n",
      "[epoch 2, pattern number 3608] loss: 0.008\n",
      "[epoch 2, pattern number 3384] loss: 0.007\n",
      "[epoch 2, pattern number 2634] loss: 0.004\n",
      "[epoch 2, pattern number 2786] loss: 0.008\n",
      "[epoch 2, pattern number 2433] loss: 0.015\n",
      "[epoch 2, pattern number 1302] loss: 0.008\n",
      "[epoch 2, pattern number 2946] loss: 0.012\n",
      "[epoch 2, pattern number 230] loss: 0.010\n",
      "[epoch 2, pattern number 2417] loss: 0.008\n",
      "[epoch 2, pattern number 1715] loss: 0.006\n",
      "[epoch 2, pattern number 1402] loss: 0.005\n",
      "[epoch 2, pattern number 2738] loss: 0.006\n",
      "[epoch 2, pattern number 1186] loss: 0.005\n",
      "[epoch 2, pattern number 2028] loss: 0.007\n",
      "[epoch 2, pattern number 1925] loss: 0.003\n",
      "[epoch 2, pattern number 145] loss: 0.004\n",
      "[epoch 2, pattern number 1977] loss: 0.006\n",
      "[epoch 2, pattern number 3314] loss: 0.008\n",
      "[epoch 2, pattern number 1145] loss: 0.004\n",
      "[epoch 2, pattern number 735] loss: 0.006\n",
      "[epoch 2, pattern number 2961] loss: 0.011\n",
      "[epoch 2, pattern number 269] loss: 0.005\n",
      "[epoch 2, pattern number 2061] loss: 0.006\n",
      "[epoch 2, pattern number 3972] loss: 0.012\n",
      "[epoch 2, pattern number 2168] loss: 0.008\n",
      "[epoch 2, pattern number 3225] loss: 0.006\n",
      "[epoch 2, pattern number 3551] loss: 0.008\n",
      "[epoch 2, pattern number 1881] loss: 0.005\n",
      "[epoch 2, pattern number 219] loss: 0.005\n",
      "[epoch 2, pattern number 3963] loss: 0.006\n",
      "[epoch 2, pattern number 1213] loss: 0.007\n",
      "[epoch 2, pattern number 2441] loss: 0.008\n",
      "[epoch 2, pattern number 3686] loss: 0.006\n",
      "[epoch 2, pattern number 558] loss: 0.006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 2, pattern number 2470] loss: 0.003\n",
      "[epoch 2, pattern number 3223] loss: 0.008\n",
      "[epoch 2, pattern number 1736] loss: 0.006\n",
      "[epoch 2, pattern number 384] loss: 0.007\n",
      "[epoch 2, pattern number 1770] loss: 0.007\n",
      "[epoch 2, pattern number 3308] loss: 0.007\n",
      "[epoch 2, pattern number 3727] loss: 0.007\n",
      "Finished Training\n",
      "Validating in fold number: 2\n",
      "[fold number 2, pattern number 1459] current (single pattern) loss: 0.033\n",
      "[fold number 2, pattern number 819] current (single pattern) loss: 0.003\n",
      "[fold number 2, pattern number 3139] current (single pattern) loss: 0.005\n",
      "[fold number 2, pattern number 539] current (single pattern) loss: 0.000\n",
      "[fold number 2, pattern number 1559] current (single pattern) loss: 0.017\n",
      "[fold number 2, pattern number 2099] current (single pattern) loss: 0.009\n",
      "[fold number 2, pattern number 579] current (single pattern) loss: 0.010\n",
      "[fold number 2, pattern number 879] current (single pattern) loss: 0.017\n",
      "[fold number 2, pattern number 3239] current (single pattern) loss: 0.015\n",
      "[fold number 2, pattern number 2239] current (single pattern) loss: 0.013\n",
      "[fold number 2, pattern number 3879] current (single pattern) loss: 0.005\n",
      "[fold number 2, pattern number 3779] current (single pattern) loss: 0.000\n",
      "[fold number 2, pattern number 2459] current (single pattern) loss: 0.017\n",
      "[fold number 2, pattern number 3039] current (single pattern) loss: 0.010\n",
      "[fold number 2, pattern number 2399] current (single pattern) loss: 0.001\n",
      "[fold number 2, pattern number 3899] current (single pattern) loss: 0.000\n",
      "[fold number 2, pattern number 339] current (single pattern) loss: 0.002\n",
      "[fold number 2, pattern number 119] current (single pattern) loss: 0.007\n",
      "[fold number 2, pattern number 3699] current (single pattern) loss: 0.001\n",
      "[fold number 2, pattern number 3079] current (single pattern) loss: 0.002\n",
      "[fold number 2, pattern number 3439] current (single pattern) loss: 0.003\n",
      "[fold number 2, pattern number 39] current (single pattern) loss: 0.001\n",
      "[fold number 2, pattern number 1719] current (single pattern) loss: 0.002\n",
      "[fold number 2, pattern number 159] current (single pattern) loss: 0.003\n",
      "[fold number 2, pattern number 2819] current (single pattern) loss: 0.000\n",
      "[fold number 2, pattern number 999] current (single pattern) loss: 0.000\n",
      "[fold number 2, pattern number 2959] current (single pattern) loss: 0.000\n",
      "[fold number 2, pattern number 619] current (single pattern) loss: 0.001\n",
      "[fold number 2, pattern number 3219] current (single pattern) loss: 0.002\n",
      "[fold number 2, pattern number 2059] current (single pattern) loss: 0.001\n",
      "[fold number 2, pattern number 1379] current (single pattern) loss: 0.001\n",
      "[fold number 2, pattern number 739] current (single pattern) loss: 0.011\n",
      "[fold number 2, pattern number 3059] current (single pattern) loss: 0.023\n",
      "[fold number 2, pattern number 1259] current (single pattern) loss: 0.004\n",
      "[fold number 2, pattern number 59] current (single pattern) loss: 0.001\n",
      "[fold number 2, pattern number 2679] current (single pattern) loss: 0.000\n",
      "[fold number 2, pattern number 1339] current (single pattern) loss: 0.006\n",
      "[fold number 2, pattern number 1839] current (single pattern) loss: 0.014\n",
      "[fold number 2, pattern number 3719] current (single pattern) loss: 0.005\n",
      "[fold number 2, pattern number 3399] current (single pattern) loss: 0.004\n",
      "[fold number 2, pattern number 1619] current (single pattern) loss: 0.029\n",
      "[fold number 2, pattern number 259] current (single pattern) loss: 0.003\n",
      "[fold number 2, pattern number 1399] current (single pattern) loss: 0.007\n",
      "[fold number 2, pattern number 3659] current (single pattern) loss: 0.000\n",
      "[fold number 2, pattern number 519] current (single pattern) loss: 0.003\n",
      "[fold number 2, pattern number 2799] current (single pattern) loss: 0.025\n",
      "[fold number 2, pattern number 2199] current (single pattern) loss: 0.003\n",
      "[fold number 2, pattern number 3199] current (single pattern) loss: 0.000\n",
      "[fold number 2, pattern number 1859] current (single pattern) loss: 0.005\n",
      "[fold number 2, pattern number 3839] current (single pattern) loss: 0.009\n",
      "[fold number 2, pattern number 1019] current (single pattern) loss: 0.022\n",
      "[fold number 2, pattern number 2639] current (single pattern) loss: 0.003\n",
      "[fold number 2, pattern number 1759] current (single pattern) loss: 0.000\n",
      "[fold number 2, pattern number 1939] current (single pattern) loss: 0.023\n",
      "[fold number 2, pattern number 3379] current (single pattern) loss: 0.000\n",
      "[fold number 2, pattern number 939] current (single pattern) loss: 0.000\n",
      "[fold number 2, pattern number 1059] current (single pattern) loss: 0.000\n",
      "[fold number 2, pattern number 2899] current (single pattern) loss: 0.023\n",
      "[fold number 2, pattern number 2859] current (single pattern) loss: 0.001\n",
      "[fold number 2, pattern number 3759] current (single pattern) loss: 0.003\n",
      "[fold number 2, pattern number 3999] current (single pattern) loss: 0.001\n",
      "[fold number 2, pattern number 319] current (single pattern) loss: 0.000\n",
      "[fold number 2, pattern number 1099] current (single pattern) loss: 0.006\n",
      "[fold number 2, pattern number 3319] current (single pattern) loss: 0.006\n",
      "[fold number 2, pattern number 359] current (single pattern) loss: 0.004\n",
      "[fold number 2, pattern number 1139] current (single pattern) loss: 0.001\n",
      "[fold number 2, pattern number 1479] current (single pattern) loss: 0.006\n",
      "[fold number 2, pattern number 279] current (single pattern) loss: 0.001\n",
      "[fold number 2, pattern number 779] current (single pattern) loss: 0.012\n",
      "[fold number 2, pattern number 2259] current (single pattern) loss: 0.014\n",
      "[fold number 2, pattern number 3559] current (single pattern) loss: 0.004\n",
      "[fold number 2, pattern number 2219] current (single pattern) loss: 0.001\n",
      "[fold number 2, pattern number 3279] current (single pattern) loss: 0.000\n",
      "Training in fold number: 3\n",
      "[epoch 1, pattern number 38] loss: 0.336\n",
      "[epoch 1, pattern number 1901] loss: 0.136\n",
      "[epoch 1, pattern number 638] loss: 0.122\n",
      "[epoch 1, pattern number 3112] loss: 0.094\n",
      "[epoch 1, pattern number 1392] loss: 0.046\n",
      "[epoch 1, pattern number 3701] loss: 0.040\n",
      "[epoch 1, pattern number 3620] loss: 0.022\n",
      "[epoch 1, pattern number 1695] loss: 0.028\n",
      "[epoch 1, pattern number 3774] loss: 0.057\n",
      "[epoch 1, pattern number 2894] loss: 0.008\n",
      "[epoch 1, pattern number 1028] loss: 0.022\n",
      "[epoch 1, pattern number 2170] loss: 0.021\n",
      "[epoch 1, pattern number 3919] loss: 0.014\n",
      "[epoch 1, pattern number 1741] loss: 0.011\n",
      "[epoch 1, pattern number 2407] loss: 0.012\n",
      "[epoch 1, pattern number 3188] loss: 0.009\n",
      "[epoch 1, pattern number 807] loss: 0.009\n",
      "[epoch 1, pattern number 3082] loss: 0.005\n",
      "[epoch 1, pattern number 402] loss: 0.020\n",
      "[epoch 1, pattern number 3204] loss: 0.010\n",
      "[epoch 1, pattern number 2935] loss: 0.017\n",
      "[epoch 1, pattern number 3820] loss: 0.007\n",
      "[epoch 1, pattern number 1890] loss: 0.008\n",
      "[epoch 1, pattern number 769] loss: 0.007\n",
      "[epoch 1, pattern number 2577] loss: 0.008\n",
      "[epoch 1, pattern number 1531] loss: 0.004\n",
      "[epoch 1, pattern number 2399] loss: 0.006\n",
      "[epoch 1, pattern number 1862] loss: 0.013\n",
      "[epoch 1, pattern number 412] loss: 0.007\n",
      "[epoch 1, pattern number 3581] loss: 0.009\n",
      "[epoch 1, pattern number 366] loss: 0.007\n",
      "[epoch 1, pattern number 3831] loss: 0.011\n",
      "[epoch 1, pattern number 2179] loss: 0.005\n",
      "[epoch 1, pattern number 1740] loss: 0.006\n",
      "[epoch 1, pattern number 2781] loss: 0.007\n",
      "[epoch 1, pattern number 481] loss: 0.016\n",
      "[epoch 1, pattern number 738] loss: 0.007\n",
      "[epoch 1, pattern number 3048] loss: 0.007\n",
      "[epoch 1, pattern number 3776] loss: 0.006\n",
      "[epoch 1, pattern number 840] loss: 0.006\n",
      "[epoch 1, pattern number 591] loss: 0.010\n",
      "[epoch 1, pattern number 652] loss: 0.009\n",
      "[epoch 1, pattern number 232] loss: 0.012\n",
      "[epoch 1, pattern number 3570] loss: 0.012\n",
      "[epoch 1, pattern number 2661] loss: 0.017\n",
      "[epoch 1, pattern number 1103] loss: 0.010\n",
      "[epoch 1, pattern number 1898] loss: 0.006\n",
      "[epoch 1, pattern number 2465] loss: 0.008\n",
      "[epoch 1, pattern number 752] loss: 0.010\n",
      "[epoch 1, pattern number 1013] loss: 0.010\n",
      "[epoch 1, pattern number 3366] loss: 0.009\n",
      "[epoch 1, pattern number 3956] loss: 0.010\n",
      "[epoch 1, pattern number 3468] loss: 0.007\n",
      "[epoch 1, pattern number 558] loss: 0.006\n",
      "[epoch 1, pattern number 2102] loss: 0.007\n",
      "[epoch 1, pattern number 3962] loss: 0.010\n",
      "[epoch 1, pattern number 257] loss: 0.004\n",
      "[epoch 1, pattern number 3535] loss: 0.010\n",
      "[epoch 1, pattern number 559] loss: 0.008\n",
      "[epoch 1, pattern number 3496] loss: 0.008\n",
      "[epoch 1, pattern number 557] loss: 0.009\n",
      "[epoch 1, pattern number 2167] loss: 0.006\n",
      "[epoch 1, pattern number 856] loss: 0.006\n",
      "[epoch 1, pattern number 827] loss: 0.009\n",
      "[epoch 1, pattern number 2385] loss: 0.009\n",
      "[epoch 1, pattern number 1182] loss: 0.008\n",
      "[epoch 1, pattern number 3597] loss: 0.008\n",
      "[epoch 1, pattern number 2801] loss: 0.007\n",
      "[epoch 1, pattern number 1154] loss: 0.010\n",
      "[epoch 1, pattern number 929] loss: 0.007\n",
      "[epoch 1, pattern number 1492] loss: 0.006\n",
      "[epoch 1, pattern number 3981] loss: 0.006\n",
      "[epoch 1, pattern number 3434] loss: 0.011\n",
      "[epoch 1, pattern number 702] loss: 0.011\n",
      "[epoch 1, pattern number 1511] loss: 0.006\n",
      "[epoch 1, pattern number 944] loss: 0.010\n",
      "[epoch 1, pattern number 1702] loss: 0.007\n",
      "[epoch 1, pattern number 782] loss: 0.009\n",
      "[epoch 1, pattern number 2342] loss: 0.004\n",
      "[epoch 1, pattern number 2881] loss: 0.006\n",
      "[epoch 1, pattern number 3949] loss: 0.007\n",
      "[epoch 1, pattern number 3201] loss: 0.008\n",
      "[epoch 1, pattern number 905] loss: 0.006\n",
      "[epoch 1, pattern number 1928] loss: 0.006\n",
      "[epoch 1, pattern number 3279] loss: 0.008\n",
      "[epoch 1, pattern number 3864] loss: 0.004\n",
      "[epoch 1, pattern number 3513] loss: 0.007\n",
      "[epoch 1, pattern number 1594] loss: 0.012\n",
      "[epoch 1, pattern number 1477] loss: 0.012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1, pattern number 3605] loss: 0.006\n",
      "[epoch 1, pattern number 1137] loss: 0.005\n",
      "[epoch 1, pattern number 3077] loss: 0.007\n",
      "[epoch 1, pattern number 2384] loss: 0.013\n",
      "[epoch 1, pattern number 395] loss: 0.006\n",
      "[epoch 1, pattern number 244] loss: 0.009\n",
      "[epoch 1, pattern number 3236] loss: 0.007\n",
      "[epoch 1, pattern number 27] loss: 0.009\n",
      "[epoch 1, pattern number 537] loss: 0.006\n",
      "[epoch 1, pattern number 1055] loss: 0.005\n",
      "[epoch 1, pattern number 1689] loss: 0.006\n",
      "[epoch 1, pattern number 3912] loss: 0.007\n",
      "[epoch 1, pattern number 1069] loss: 0.007\n",
      "[epoch 1, pattern number 433] loss: 0.011\n",
      "[epoch 1, pattern number 2675] loss: 0.006\n",
      "[epoch 1, pattern number 3931] loss: 0.006\n",
      "[epoch 1, pattern number 2088] loss: 0.005\n",
      "[epoch 1, pattern number 424] loss: 0.008\n",
      "[epoch 1, pattern number 44] loss: 0.005\n",
      "[epoch 1, pattern number 2889] loss: 0.006\n",
      "[epoch 1, pattern number 2301] loss: 0.004\n",
      "[epoch 1, pattern number 2338] loss: 0.004\n",
      "[epoch 1, pattern number 2420] loss: 0.006\n",
      "[epoch 1, pattern number 2322] loss: 0.009\n",
      "[epoch 1, pattern number 3430] loss: 0.007\n",
      "[epoch 1, pattern number 1217] loss: 0.008\n",
      "[epoch 1, pattern number 2908] loss: 0.007\n",
      "[epoch 1, pattern number 3087] loss: 0.010\n",
      "[epoch 1, pattern number 1763] loss: 0.004\n",
      "[epoch 1, pattern number 2504] loss: 0.006\n",
      "[epoch 1, pattern number 47] loss: 0.006\n",
      "[epoch 1, pattern number 3152] loss: 0.006\n",
      "[epoch 1, pattern number 3955] loss: 0.011\n",
      "[epoch 1, pattern number 1046] loss: 0.009\n",
      "[epoch 1, pattern number 1573] loss: 0.006\n",
      "[epoch 1, pattern number 2205] loss: 0.008\n",
      "[epoch 1, pattern number 3133] loss: 0.008\n",
      "[epoch 1, pattern number 1497] loss: 0.005\n",
      "[epoch 1, pattern number 1298] loss: 0.005\n",
      "[epoch 1, pattern number 1443] loss: 0.007\n",
      "[epoch 1, pattern number 97] loss: 0.005\n",
      "[epoch 1, pattern number 2697] loss: 0.003\n",
      "[epoch 1, pattern number 3910] loss: 0.006\n",
      "[epoch 1, pattern number 3150] loss: 0.008\n",
      "[epoch 2, pattern number 313] loss: 0.006\n",
      "[epoch 2, pattern number 2085] loss: 0.004\n",
      "[epoch 2, pattern number 1926] loss: 0.004\n",
      "[epoch 2, pattern number 891] loss: 0.004\n",
      "[epoch 2, pattern number 3740] loss: 0.010\n",
      "[epoch 2, pattern number 3469] loss: 0.003\n",
      "[epoch 2, pattern number 2210] loss: 0.005\n",
      "[epoch 2, pattern number 281] loss: 0.005\n",
      "[epoch 2, pattern number 3502] loss: 0.005\n",
      "[epoch 2, pattern number 3436] loss: 0.009\n",
      "[epoch 2, pattern number 901] loss: 0.006\n",
      "[epoch 2, pattern number 3542] loss: 0.006\n",
      "[epoch 2, pattern number 2301] loss: 0.004\n",
      "[epoch 2, pattern number 125] loss: 0.004\n",
      "[epoch 2, pattern number 1421] loss: 0.011\n",
      "[epoch 2, pattern number 3938] loss: 0.004\n",
      "[epoch 2, pattern number 667] loss: 0.006\n",
      "[epoch 2, pattern number 3697] loss: 0.012\n",
      "[epoch 2, pattern number 1807] loss: 0.003\n",
      "[epoch 2, pattern number 3497] loss: 0.006\n",
      "[epoch 2, pattern number 2438] loss: 0.011\n",
      "[epoch 2, pattern number 459] loss: 0.005\n",
      "[epoch 2, pattern number 1265] loss: 0.010\n",
      "[epoch 2, pattern number 3851] loss: 0.008\n",
      "[epoch 2, pattern number 2789] loss: 0.004\n",
      "[epoch 2, pattern number 1828] loss: 0.006\n",
      "[epoch 2, pattern number 3425] loss: 0.004\n",
      "[epoch 2, pattern number 1290] loss: 0.007\n",
      "[epoch 2, pattern number 1906] loss: 0.005\n",
      "[epoch 2, pattern number 881] loss: 0.005\n",
      "[epoch 2, pattern number 28] loss: 0.007\n",
      "[epoch 2, pattern number 1970] loss: 0.007\n",
      "[epoch 2, pattern number 129] loss: 0.008\n",
      "[epoch 2, pattern number 1584] loss: 0.006\n",
      "[epoch 2, pattern number 796] loss: 0.005\n",
      "[epoch 2, pattern number 932] loss: 0.006\n",
      "[epoch 2, pattern number 1774] loss: 0.007\n",
      "[epoch 2, pattern number 3372] loss: 0.008\n",
      "[epoch 2, pattern number 1259] loss: 0.007\n",
      "[epoch 2, pattern number 2253] loss: 0.006\n",
      "[epoch 2, pattern number 3615] loss: 0.006\n",
      "[epoch 2, pattern number 1191] loss: 0.005\n",
      "[epoch 2, pattern number 652] loss: 0.003\n",
      "[epoch 2, pattern number 2278] loss: 0.005\n",
      "[epoch 2, pattern number 1171] loss: 0.006\n",
      "[epoch 2, pattern number 3173] loss: 0.007\n",
      "[epoch 2, pattern number 3678] loss: 0.007\n",
      "[epoch 2, pattern number 1051] loss: 0.005\n",
      "[epoch 2, pattern number 606] loss: 0.007\n",
      "[epoch 2, pattern number 1627] loss: 0.004\n",
      "[epoch 2, pattern number 2053] loss: 0.005\n",
      "[epoch 2, pattern number 50] loss: 0.003\n",
      "[epoch 2, pattern number 302] loss: 0.008\n",
      "[epoch 2, pattern number 3102] loss: 0.006\n",
      "[epoch 2, pattern number 3480] loss: 0.008\n",
      "[epoch 2, pattern number 464] loss: 0.010\n",
      "[epoch 2, pattern number 2379] loss: 0.010\n",
      "[epoch 2, pattern number 3709] loss: 0.005\n",
      "[epoch 2, pattern number 3597] loss: 0.006\n",
      "[epoch 2, pattern number 3119] loss: 0.007\n",
      "[epoch 2, pattern number 499] loss: 0.006\n",
      "[epoch 2, pattern number 789] loss: 0.016\n",
      "[epoch 2, pattern number 3394] loss: 0.006\n",
      "[epoch 2, pattern number 2939] loss: 0.007\n",
      "[epoch 2, pattern number 307] loss: 0.008\n",
      "[epoch 2, pattern number 696] loss: 0.010\n",
      "[epoch 2, pattern number 2180] loss: 0.013\n",
      "[epoch 2, pattern number 3910] loss: 0.010\n",
      "[epoch 2, pattern number 1113] loss: 0.007\n",
      "[epoch 2, pattern number 2314] loss: 0.007\n",
      "[epoch 2, pattern number 949] loss: 0.008\n",
      "[epoch 2, pattern number 2125] loss: 0.005\n",
      "[epoch 2, pattern number 564] loss: 0.006\n",
      "[epoch 2, pattern number 834] loss: 0.004\n",
      "[epoch 2, pattern number 1403] loss: 0.005\n",
      "[epoch 2, pattern number 192] loss: 0.004\n",
      "[epoch 2, pattern number 2732] loss: 0.005\n",
      "[epoch 2, pattern number 2669] loss: 0.007\n",
      "[epoch 2, pattern number 3415] loss: 0.007\n",
      "[epoch 2, pattern number 2821] loss: 0.004\n",
      "[epoch 2, pattern number 2958] loss: 0.009\n",
      "[epoch 2, pattern number 2266] loss: 0.007\n",
      "[epoch 2, pattern number 1384] loss: 0.006\n",
      "[epoch 2, pattern number 1936] loss: 0.007\n",
      "[epoch 2, pattern number 3590] loss: 0.006\n",
      "[epoch 2, pattern number 778] loss: 0.006\n",
      "[epoch 2, pattern number 926] loss: 0.013\n",
      "[epoch 2, pattern number 1636] loss: 0.004\n",
      "[epoch 2, pattern number 739] loss: 0.009\n",
      "[epoch 2, pattern number 583] loss: 0.006\n",
      "[epoch 2, pattern number 3278] loss: 0.010\n",
      "[epoch 2, pattern number 3375] loss: 0.008\n",
      "[epoch 2, pattern number 2998] loss: 0.010\n",
      "[epoch 2, pattern number 2058] loss: 0.008\n",
      "[epoch 2, pattern number 3399] loss: 0.009\n",
      "[epoch 2, pattern number 2091] loss: 0.013\n",
      "[epoch 2, pattern number 3139] loss: 0.012\n",
      "[epoch 2, pattern number 1836] loss: 0.005\n",
      "[epoch 2, pattern number 798] loss: 0.008\n",
      "[epoch 2, pattern number 1730] loss: 0.006\n",
      "[epoch 2, pattern number 3888] loss: 0.005\n",
      "[epoch 2, pattern number 3905] loss: 0.005\n",
      "[epoch 2, pattern number 3196] loss: 0.008\n",
      "[epoch 2, pattern number 3570] loss: 0.011\n",
      "[epoch 2, pattern number 867] loss: 0.006\n",
      "[epoch 2, pattern number 3078] loss: 0.006\n",
      "[epoch 2, pattern number 3656] loss: 0.008\n",
      "[epoch 2, pattern number 397] loss: 0.005\n",
      "[epoch 2, pattern number 658] loss: 0.011\n",
      "[epoch 2, pattern number 2167] loss: 0.007\n",
      "[epoch 2, pattern number 1249] loss: 0.008\n",
      "[epoch 2, pattern number 1394] loss: 0.010\n",
      "[epoch 2, pattern number 3137] loss: 0.010\n",
      "[epoch 2, pattern number 3248] loss: 0.007\n",
      "[epoch 2, pattern number 2845] loss: 0.005\n",
      "[epoch 2, pattern number 740] loss: 0.006\n",
      "[epoch 2, pattern number 1722] loss: 0.004\n",
      "[epoch 2, pattern number 62] loss: 0.006\n",
      "[epoch 2, pattern number 548] loss: 0.006\n",
      "[epoch 2, pattern number 584] loss: 0.003\n",
      "[epoch 2, pattern number 3679] loss: 0.015\n",
      "[epoch 2, pattern number 608] loss: 0.008\n",
      "[epoch 2, pattern number 2149] loss: 0.005\n",
      "[epoch 2, pattern number 1681] loss: 0.007\n",
      "[epoch 2, pattern number 637] loss: 0.008\n",
      "[epoch 2, pattern number 2491] loss: 0.013\n",
      "[epoch 2, pattern number 157] loss: 0.007\n",
      "[epoch 2, pattern number 2080] loss: 0.008\n",
      "[epoch 2, pattern number 2230] loss: 0.008\n",
      "[epoch 2, pattern number 2950] loss: 0.008\n",
      "[epoch 2, pattern number 2172] loss: 0.007\n",
      "[epoch 2, pattern number 451] loss: 0.005\n",
      "[epoch 2, pattern number 3743] loss: 0.009\n",
      "Finished Training\n",
      "Validating in fold number: 3\n",
      "[fold number 3, pattern number 2879] current (single pattern) loss: 0.004\n",
      "[fold number 3, pattern number 3819] current (single pattern) loss: 0.015\n",
      "[fold number 3, pattern number 2339] current (single pattern) loss: 0.000\n",
      "[fold number 3, pattern number 379] current (single pattern) loss: 0.001\n",
      "[fold number 3, pattern number 699] current (single pattern) loss: 0.013\n",
      "[fold number 3, pattern number 1279] current (single pattern) loss: 0.004\n",
      "[fold number 3, pattern number 219] current (single pattern) loss: 0.014\n",
      "[fold number 3, pattern number 2279] current (single pattern) loss: 0.011\n",
      "[fold number 3, pattern number 3939] current (single pattern) loss: 0.016\n",
      "[fold number 3, pattern number 3979] current (single pattern) loss: 0.057\n",
      "[fold number 3, pattern number 2479] current (single pattern) loss: 0.005\n",
      "[fold number 3, pattern number 3259] current (single pattern) loss: 0.001\n",
      "[fold number 3, pattern number 3339] current (single pattern) loss: 0.000\n",
      "[fold number 3, pattern number 1739] current (single pattern) loss: 0.010\n",
      "[fold number 3, pattern number 2979] current (single pattern) loss: 0.003\n",
      "[fold number 3, pattern number 179] current (single pattern) loss: 0.013\n",
      "[fold number 3, pattern number 1359] current (single pattern) loss: 0.001\n",
      "[fold number 3, pattern number 2299] current (single pattern) loss: 0.008\n",
      "[fold number 3, pattern number 1679] current (single pattern) loss: 0.000\n",
      "[fold number 3, pattern number 3959] current (single pattern) loss: 0.011\n",
      "[fold number 3, pattern number 1079] current (single pattern) loss: 0.000\n",
      "[fold number 3, pattern number 3799] current (single pattern) loss: 0.006\n",
      "[fold number 3, pattern number 839] current (single pattern) loss: 0.012\n",
      "[fold number 3, pattern number 3619] current (single pattern) loss: 0.009\n",
      "[fold number 3, pattern number 1959] current (single pattern) loss: 0.005\n",
      "[fold number 3, pattern number 1299] current (single pattern) loss: 0.002\n",
      "[fold number 3, pattern number 3599] current (single pattern) loss: 0.001\n",
      "[fold number 3, pattern number 239] current (single pattern) loss: 0.002\n",
      "[fold number 3, pattern number 1899] current (single pattern) loss: 0.004\n",
      "[fold number 3, pattern number 2659] current (single pattern) loss: 0.021\n",
      "[fold number 3, pattern number 1599] current (single pattern) loss: 0.003\n",
      "[fold number 3, pattern number 1579] current (single pattern) loss: 0.008\n",
      "[fold number 3, pattern number 2719] current (single pattern) loss: 0.001\n",
      "[fold number 3, pattern number 959] current (single pattern) loss: 0.001\n",
      "[fold number 3, pattern number 859] current (single pattern) loss: 0.003\n",
      "[fold number 3, pattern number 2419] current (single pattern) loss: 0.005\n",
      "[fold number 3, pattern number 3359] current (single pattern) loss: 0.005\n",
      "[fold number 3, pattern number 2839] current (single pattern) loss: 0.005\n",
      "[fold number 3, pattern number 2559] current (single pattern) loss: 0.006\n",
      "[fold number 3, pattern number 599] current (single pattern) loss: 0.017\n",
      "[fold number 3, pattern number 1159] current (single pattern) loss: 0.001\n",
      "[fold number 3, pattern number 3479] current (single pattern) loss: 0.000\n",
      "[fold number 3, pattern number 1039] current (single pattern) loss: 0.000\n",
      "[fold number 3, pattern number 419] current (single pattern) loss: 0.000\n",
      "[fold number 3, pattern number 1519] current (single pattern) loss: 0.000\n",
      "[fold number 3, pattern number 1819] current (single pattern) loss: 0.000\n",
      "[fold number 3, pattern number 2759] current (single pattern) loss: 0.001\n",
      "[fold number 3, pattern number 919] current (single pattern) loss: 0.000\n",
      "[fold number 3, pattern number 1659] current (single pattern) loss: 0.000\n",
      "[fold number 3, pattern number 1499] current (single pattern) loss: 0.000\n",
      "[fold number 3, pattern number 3299] current (single pattern) loss: 0.002\n",
      "[fold number 3, pattern number 2739] current (single pattern) loss: 0.000\n",
      "[fold number 3, pattern number 2319] current (single pattern) loss: 0.007\n",
      "[fold number 3, pattern number 2579] current (single pattern) loss: 0.019\n",
      "[fold number 3, pattern number 139] current (single pattern) loss: 0.002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fold number 3, pattern number 2999] current (single pattern) loss: 0.003\n",
      "Finished fold iterations\n"
     ]
    }
   ],
   "source": [
    "    from IPython.core.display import display, HTML\n",
    "    display(HTML(\"<style>div.output_scroll { height: 44em; }</style>\"))\n",
    "    \n",
    "    fold = 0\n",
    "    loss_function = torch.nn.MSELoss()\n",
    "\n",
    "    for train_index, test_index in kf.split(indices):\n",
    "        fold += 1\n",
    "        print(\"Training in fold number:\", fold)\n",
    "        \n",
    "        # Define the network for this fold. It is a kind of weight reset.\n",
    "        # In more complex scenarios we could use different ANN for every fold.\n",
    "        # For example, assuming there is a function taking an integer and\n",
    "        # returning a network we could make net = get_network_for_fold(fold)\n",
    "        net = Net(n_feature=1, n_hidden=10, n_output=1)\n",
    "        # print(net)  # net architecture\n",
    "    \n",
    "        # We globaly define the hyperparamers but they could be paramerters \n",
    "        # of the training algo.\n",
    "        epochs = 2\n",
    "        optimizer = torch.optim.SGD(net.parameters(), lr=0.2)\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        \n",
    "        current_training_d_without_d_i = SubsetRandomSampler(\n",
    "                indices=train_index)\n",
    "        \n",
    "        current_training_d_loader = torch.utils.data.DataLoader(dataset, \n",
    "                                                            batch_size=1, \n",
    "                                                            shuffle=False, \n",
    "                                        sampler=current_training_d_without_d_i, \n",
    "                                                            batch_sampler=None,\n",
    "                                                            num_workers=0,\n",
    "                                                            collate_fn=None,\n",
    "                                                            pin_memory=False,\n",
    "                                                            drop_last=False,\n",
    "                                                            timeout=0,\n",
    "                                                        worker_init_fn=None, \n",
    "                                                multiprocessing_context=None)\n",
    "        \n",
    "        current_d_i = SubsetRandomSampler(indices=test_index)\n",
    "        \n",
    "        current_d_i_loader = torch.utils.data.DataLoader(dataset, \n",
    "                                                            batch_size=1, \n",
    "                                                            shuffle=False, \n",
    "                                                        sampler=current_d_i, \n",
    "                                                            batch_sampler=None,\n",
    "                                                            num_workers=0,\n",
    "                                                            collate_fn=None,\n",
    "                                                            pin_memory=False,\n",
    "                                                            drop_last=False,\n",
    "                                                            timeout=0,\n",
    "                                                        worker_init_fn=None, \n",
    "                                                multiprocessing_context=None)\n",
    "        # train CNN\n",
    "        # $f_i$ is the learning algorithm. In this case, is the ANN with the \n",
    "        # \"best parameters\" according to the loss function used inside the\n",
    "        # training loop. Note that network architecture, loss function \n",
    "        # (criterion) and number of iterations (epochs) remain constant.\n",
    "        # However, these paramters could be changed to perform a model \n",
    "        # selection/evaluation.\n",
    "        train_ann(ann=net, dataloader=current_training_d_loader, \n",
    "                  criterion=criterion, epochs=epochs)\n",
    "        \n",
    "        f_i = net\n",
    "    \n",
    "        # Calculate loss of the trained model output and the data elements of\n",
    "        # the current partition. Note that we could use now a different loss\n",
    "        # function than the one used to train the network itself. Nevertheless,\n",
    "        # I use the same here (L1 loss).\n",
    "        current_loss = 0.0\n",
    "        print(\"Validating in fold number:\", fold)\n",
    "        i = 0\n",
    "        for data, target, index in current_d_i_loader:\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs = data\n",
    "            labels = target\n",
    "    \n",
    "            # only forward because we are performing evaluation\n",
    "            outputs = net(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "    \n",
    "            # print statistics\n",
    "            current_loss = loss.item()\n",
    "            # i is the pattern/example index\n",
    "            error_vector[i] = current_loss\n",
    "            if index % 20 == 19:    # print every 20 examples\n",
    "                print('[fold number %d, pattern number %d] current (single pattern) loss: %.3f' %\n",
    "                      (fold, index, current_loss))\n",
    "            i += 1\n",
    "    print(\"Finished fold iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
